{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_dachshund.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEEx4lfQvASTl4V7Ck8k0b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konkuk-gaegul/3rd-Team-Project/blob/main/Pytorch_dachshund.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 닥스훈트 학습\n",
        "- 9개 이미지 전처리 중, 가장 좋은 성능인 set 9을 대표로 업로드\n",
        "- Loss : 0.1858, Acc : 95.56%"
      ],
      "metadata": {
        "id": "kuywuEiCaB38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 전처리 설명\n",
        "**set_1**\n",
        "- 이미지 크기, 선명하게, 스케일링\n",
        "\n",
        "**set_2**\n",
        "- 이미지 크기, 선명하게, 대비향상, 스케일링\n",
        "\n",
        "**set_3**\n",
        "- 이미지 크기, 흑백, 선명하게, 대비향상, 이진화, 스케일링\n",
        "\n",
        "**set_4**\n",
        "- 이미지크기, 대비향상(RGB적용X), 스케일링\n",
        "\n",
        "**set_5**\n",
        "- 이미지크기, 양방향필터, 이진화, 스케일링\n",
        "\n",
        "**set_6**\n",
        "- 대비향상(YUV 적용), 선명도, 이미지크기x, 스케일링x\n",
        "\n",
        "**set_7**\n",
        "- 이미지크기, 배경제거(높이150), 스케일링x\n",
        "\n",
        "**set_8**\n",
        "- 이미지크기, 선명도, 배경제거(높이180), 스케일링x\n",
        "\n",
        "**set_9**\n",
        "- 정방형자르기, 대비향상(YUV 적용), 선명도, 스케일링x"
      ],
      "metadata": {
        "id": "JkzwtRwHDXnS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39a3b0f7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import copy\n",
        "import os, shutil\n",
        "import matplotlib as plt\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device 객체"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "697e6051",
        "outputId": "c43dea2f-cf25-473c-ec1d-b5990c1410ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "닥스훈트 set_9\n",
            "698\n",
            "299\n",
            "---------\n"
          ]
        }
      ],
      "source": [
        "for i in range(9, 10):\n",
        "    base_dir = '/home/tutor/안재형/dog_pic/닥스훈트'\n",
        "    base_folder = os.path.join(base_dir, f'set_{i}')\n",
        "    # # 훈련셋, 검증셋, 테스트셋을 미리 분할하기 위한 폴더 경로\n",
        "    train_path = os.path.join(base_folder, 'train')\n",
        "    os.mkdir( train_path)\n",
        "\n",
        "    test_path = os.path.join(base_folder, 'test')\n",
        "    os.mkdir( test_path)\n",
        "\n",
        "    # # train 정상 폴더\n",
        "    train_nor_path = os.path.join(train_path, 'nor')\n",
        "    os.mkdir( train_nor_path)\n",
        "\n",
        "    # # train 비만 폴더\n",
        "    train_fat_path = os.path.join(train_path, 'fat')\n",
        "    os.mkdir( train_fat_path)\n",
        "\n",
        "    # # test 정상 폴더\n",
        "    test_nor_path = os.path.join(test_path, 'nor')\n",
        "    os.mkdir( test_nor_path)\n",
        "\n",
        "    # # test 비만 폴더\n",
        "    test_fat_path = os.path.join(test_path, 'fat')\n",
        "    os.mkdir( test_fat_path)\n",
        "    \n",
        "    nor_path = os.path.join(base_folder, '전처리_정상')\n",
        "    fat_path = os.path.join(base_folder, '전처리_비만')\n",
        "\n",
        "    nor_list = os.listdir(nor_path)\n",
        "    fat_list = os.listdir(fat_path)\n",
        "    print(f'닥스훈트 set_{i}')\n",
        "    print(len(nor_list))\n",
        "    print(len(fat_list))\n",
        "    print('---------')\n",
        "    \n",
        "    # train : 75%, test : 25%\n",
        "    # 정상 이미지 복사\n",
        "    for i in range(len(nor_list)):\n",
        "        src_path = os.path.join(nor_path, nor_list[i])\n",
        "\n",
        "        if i < len(nor_list)*0.75:\n",
        "            dst_path = os.path.join(train_nor_path, f'nor_{i}.jpg')\n",
        "            shutil.copyfile( src_path, dst_path )\n",
        "        else :\n",
        "            dst_path = os.path.join(test_nor_path, f'nor_{i}.jpg')\n",
        "            shutil.copyfile( src_path, dst_path )\n",
        "\n",
        "    # 비만 이미지 복사\n",
        "    for i in range(len(fat_list)):\n",
        "        src_path = os.path.join(fat_path, fat_list[i])\n",
        "\n",
        "        if i < len(fat_list)*0.75:\n",
        "            dst_path = os.path.join(train_fat_path, f'fat_{i}.jpg')\n",
        "            shutil.copyfile( src_path, dst_path )\n",
        "        else :\n",
        "            dst_path = os.path.join(test_fat_path, f'fat_{i}.jpg')\n",
        "            shutil.copyfile( src_path, dst_path )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7417019"
      },
      "source": [
        "# 학습 1\n",
        "- set_9 이미지 사용\n",
        "- batch_size를 5 ~ 14까지 반복, 최적의 성능을 찾는다.\n",
        "- 이미지 증식 활용\n",
        "- epoch = 50, 100, 150 각 3번 반복"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e656c56"
      },
      "outputs": [],
      "source": [
        "# Loss_Accuracy에 각 배치 사이즈마다의 성능을 넣어둔다\n",
        "Loss_Accuracy = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0c48ddd",
        "outputId": "799ed89d-11d6-4dbb-cb81-b5b1927cba72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------현재 set_9 1번째 학습-------------\n",
            "#0 Loss: 0.5076 Acc: 77.1696% Time: 6.2050s\n",
            "#1 Loss: 0.3333 Acc: 86.1148% Time: 12.0777s\n",
            "#2 Loss: 0.1885 Acc: 91.4553% Time: 17.9526s\n",
            "#3 Loss: 0.1847 Acc: 92.3899% Time: 23.6676s\n",
            "#4 Loss: 0.1641 Acc: 93.1909% Time: 29.4080s\n",
            "#5 Loss: 0.0852 Acc: 97.0628% Time: 35.1261s\n",
            "#6 Loss: 0.1400 Acc: 95.8612% Time: 40.7812s\n",
            "#7 Loss: 0.0977 Acc: 96.3952% Time: 46.8197s\n",
            "#8 Loss: 0.0543 Acc: 98.3979% Time: 52.8643s\n",
            "#9 Loss: 0.0827 Acc: 97.1963% Time: 58.7087s\n",
            "#10 Loss: 0.0563 Acc: 98.2644% Time: 64.7057s\n",
            "#11 Loss: 0.0699 Acc: 97.1963% Time: 70.6908s\n",
            "#12 Loss: 0.0841 Acc: 97.5968% Time: 76.5371s\n",
            "#13 Loss: 0.0268 Acc: 99.1989% Time: 82.3722s\n",
            "#14 Loss: 0.0475 Acc: 98.1308% Time: 88.2384s\n",
            "#15 Loss: 0.0185 Acc: 99.5995% Time: 94.0205s\n",
            "#16 Loss: 0.0368 Acc: 99.0654% Time: 99.7465s\n",
            "#17 Loss: 0.0215 Acc: 99.1989% Time: 105.9130s\n",
            "#18 Loss: 0.0314 Acc: 98.7984% Time: 111.7504s\n",
            "#19 Loss: 0.0160 Acc: 99.7330% Time: 117.6201s\n",
            "#20 Loss: 0.0123 Acc: 99.7330% Time: 123.5577s\n",
            "#21 Loss: 0.0260 Acc: 98.7984% Time: 129.3988s\n",
            "#22 Loss: 0.0151 Acc: 99.4660% Time: 135.1999s\n",
            "#23 Loss: 0.0161 Acc: 99.4660% Time: 140.9235s\n",
            "#24 Loss: 0.0184 Acc: 99.1989% Time: 146.8581s\n",
            "#25 Loss: 0.0064 Acc: 99.8665% Time: 152.5758s\n",
            "#26 Loss: 0.0081 Acc: 99.7330% Time: 158.7305s\n",
            "#27 Loss: 0.0114 Acc: 99.5995% Time: 164.3782s\n",
            "#28 Loss: 0.0146 Acc: 99.3324% Time: 170.1352s\n",
            "#29 Loss: 0.0117 Acc: 99.7330% Time: 175.8436s\n",
            "#30 Loss: 0.0086 Acc: 99.7330% Time: 181.3115s\n",
            "#31 Loss: 0.0096 Acc: 99.5995% Time: 187.0106s\n",
            "#32 Loss: 0.0210 Acc: 99.1989% Time: 192.8880s\n",
            "#33 Loss: 0.0277 Acc: 99.1989% Time: 198.6112s\n",
            "#34 Loss: 0.0351 Acc: 98.6649% Time: 204.2574s\n",
            "#35 Loss: 0.0128 Acc: 99.8665% Time: 209.8685s\n",
            "#36 Loss: 0.0040 Acc: 100.0000% Time: 215.6795s\n",
            "#37 Loss: 0.0049 Acc: 100.0000% Time: 221.6819s\n",
            "#38 Loss: 0.0076 Acc: 99.8665% Time: 227.4679s\n",
            "#39 Loss: 0.0038 Acc: 100.0000% Time: 233.3130s\n",
            "#40 Loss: 0.0025 Acc: 100.0000% Time: 239.0693s\n",
            "#41 Loss: 0.0074 Acc: 99.7330% Time: 245.1688s\n",
            "#42 Loss: 0.0088 Acc: 99.5995% Time: 251.0630s\n",
            "#43 Loss: 0.0155 Acc: 99.3324% Time: 256.8819s\n",
            "#44 Loss: 0.0026 Acc: 100.0000% Time: 262.7098s\n",
            "#45 Loss: 0.0031 Acc: 100.0000% Time: 268.5506s\n",
            "#46 Loss: 0.0020 Acc: 100.0000% Time: 274.2689s\n",
            "#47 Loss: 0.0051 Acc: 99.8665% Time: 280.0319s\n",
            "#48 Loss: 0.0029 Acc: 100.0000% Time: 285.4736s\n",
            "#49 Loss: 0.0009 Acc: 100.0000% Time: 291.2697s\n",
            "#50 Loss: 0.0009 Acc: 100.0000% Time: 297.0008s\n",
            "#51 Loss: 0.0150 Acc: 99.4660% Time: 302.7665s\n",
            "#52 Loss: 0.0022 Acc: 100.0000% Time: 308.5841s\n",
            "#53 Loss: 0.0042 Acc: 100.0000% Time: 314.2504s\n",
            "#54 Loss: 0.0074 Acc: 99.8665% Time: 320.2975s\n",
            "#55 Loss: 0.0039 Acc: 99.8665% Time: 326.2207s\n",
            "#56 Loss: 0.0032 Acc: 99.8665% Time: 332.0642s\n",
            "#57 Loss: 0.0013 Acc: 100.0000% Time: 337.9147s\n",
            "#58 Loss: 0.0033 Acc: 99.8665% Time: 343.4428s\n",
            "#59 Loss: 0.0033 Acc: 99.8665% Time: 349.0739s\n",
            "#60 Loss: 0.0017 Acc: 100.0000% Time: 355.0675s\n",
            "#61 Loss: 0.0015 Acc: 100.0000% Time: 360.9780s\n",
            "#62 Loss: 0.0042 Acc: 99.7330% Time: 366.8000s\n",
            "#63 Loss: 0.0094 Acc: 99.8665% Time: 372.7535s\n",
            "#64 Loss: 0.0028 Acc: 100.0000% Time: 378.2565s\n",
            "#65 Loss: 0.0021 Acc: 100.0000% Time: 384.2963s\n",
            "#66 Loss: 0.0012 Acc: 100.0000% Time: 390.1706s\n",
            "#67 Loss: 0.0010 Acc: 100.0000% Time: 395.6856s\n",
            "#68 Loss: 0.0006 Acc: 100.0000% Time: 401.2749s\n",
            "#69 Loss: 0.0012 Acc: 100.0000% Time: 406.8157s\n",
            "#70 Loss: 0.0022 Acc: 100.0000% Time: 412.3919s\n",
            "#71 Loss: 0.0022 Acc: 100.0000% Time: 418.2261s\n",
            "#72 Loss: 0.0017 Acc: 100.0000% Time: 423.9634s\n",
            "#73 Loss: 0.0008 Acc: 100.0000% Time: 429.7197s\n",
            "#74 Loss: 0.0035 Acc: 99.7330% Time: 435.2511s\n",
            "#75 Loss: 0.0027 Acc: 100.0000% Time: 440.9279s\n",
            "#76 Loss: 0.0011 Acc: 100.0000% Time: 446.6717s\n",
            "#77 Loss: 0.0005 Acc: 100.0000% Time: 452.2325s\n",
            "#78 Loss: 0.0009 Acc: 100.0000% Time: 457.8785s\n",
            "#79 Loss: 0.0012 Acc: 100.0000% Time: 463.5126s\n",
            "#80 Loss: 0.0009 Acc: 100.0000% Time: 469.3870s\n",
            "#81 Loss: 0.0005 Acc: 100.0000% Time: 475.3076s\n",
            "#82 Loss: 0.0015 Acc: 100.0000% Time: 481.0416s\n",
            "#83 Loss: 0.0005 Acc: 100.0000% Time: 487.1462s\n",
            "#84 Loss: 0.0006 Acc: 100.0000% Time: 492.8935s\n",
            "#85 Loss: 0.0006 Acc: 100.0000% Time: 498.5940s\n",
            "#86 Loss: 0.0226 Acc: 99.5995% Time: 504.7112s\n",
            "#87 Loss: 0.0038 Acc: 99.8665% Time: 510.5966s\n",
            "#88 Loss: 0.0026 Acc: 99.8665% Time: 516.4518s\n",
            "#89 Loss: 0.0019 Acc: 100.0000% Time: 522.1744s\n",
            "#90 Loss: 0.0019 Acc: 100.0000% Time: 527.9983s\n",
            "#91 Loss: 0.0029 Acc: 99.8665% Time: 533.7928s\n",
            "#92 Loss: 0.0034 Acc: 99.8665% Time: 539.5162s\n",
            "#93 Loss: 0.0012 Acc: 100.0000% Time: 545.2682s\n",
            "#94 Loss: 0.0014 Acc: 100.0000% Time: 551.0639s\n",
            "#95 Loss: 0.0005 Acc: 100.0000% Time: 556.9579s\n",
            "#96 Loss: 0.0009 Acc: 100.0000% Time: 562.6780s\n",
            "#97 Loss: 0.0006 Acc: 100.0000% Time: 568.5870s\n",
            "#98 Loss: 0.0014 Acc: 100.0000% Time: 574.4087s\n",
            "#99 Loss: 0.0006 Acc: 100.0000% Time: 580.2689s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.1786 Acc: 93.9516% Time: 1.0681s\n",
            "현재 Best는 epoch : 100 / iteration : 1번째 / Loss : 0.17863983664767968 \n",
            "-------------현재 set_9 2번째 학습-------------\n",
            "#0 Loss: 0.4423 Acc: 79.1722% Time: 5.9830s\n",
            "#1 Loss: 0.2781 Acc: 88.1175% Time: 11.6459s\n",
            "#2 Loss: 0.2249 Acc: 89.7196% Time: 17.4519s\n",
            "#3 Loss: 0.1459 Acc: 94.2590% Time: 23.1647s\n",
            "#4 Loss: 0.1747 Acc: 92.9239% Time: 29.2900s\n",
            "#5 Loss: 0.1003 Acc: 96.1282% Time: 35.3924s\n",
            "#6 Loss: 0.1107 Acc: 95.9947% Time: 41.2333s\n",
            "#7 Loss: 0.0990 Acc: 95.5941% Time: 46.9433s\n",
            "#8 Loss: 0.0960 Acc: 96.1282% Time: 53.0452s\n",
            "#9 Loss: 0.0668 Acc: 97.4633% Time: 58.9527s\n",
            "#10 Loss: 0.0738 Acc: 97.4633% Time: 64.7927s\n",
            "#11 Loss: 0.0619 Acc: 97.8638% Time: 70.6967s\n",
            "#12 Loss: 0.0514 Acc: 98.2644% Time: 76.5788s\n",
            "#13 Loss: 0.0443 Acc: 97.9973% Time: 82.5888s\n",
            "#14 Loss: 0.0361 Acc: 98.6649% Time: 88.6721s\n",
            "#15 Loss: 0.0778 Acc: 97.3298% Time: 94.4343s\n",
            "#16 Loss: 0.0536 Acc: 98.3979% Time: 100.1670s\n",
            "#17 Loss: 0.0258 Acc: 99.1989% Time: 105.9360s\n",
            "#18 Loss: 0.0147 Acc: 99.3324% Time: 111.7965s\n",
            "#19 Loss: 0.0513 Acc: 98.3979% Time: 117.3916s\n",
            "#20 Loss: 0.0171 Acc: 99.5995% Time: 123.2198s\n",
            "#21 Loss: 0.0186 Acc: 99.3324% Time: 128.9955s\n",
            "#22 Loss: 0.0201 Acc: 99.4660% Time: 134.8534s\n",
            "#23 Loss: 0.0076 Acc: 99.8665% Time: 140.4739s\n",
            "#24 Loss: 0.0161 Acc: 99.1989% Time: 146.2142s\n",
            "#25 Loss: 0.0100 Acc: 100.0000% Time: 152.0761s\n",
            "#26 Loss: 0.0225 Acc: 99.1989% Time: 157.8835s\n",
            "#27 Loss: 0.0090 Acc: 99.7330% Time: 163.6668s\n",
            "#28 Loss: 0.0167 Acc: 99.3324% Time: 169.7328s\n",
            "#29 Loss: 0.0065 Acc: 99.8665% Time: 175.4858s\n",
            "#30 Loss: 0.0078 Acc: 99.8665% Time: 181.4433s\n",
            "#31 Loss: 0.0050 Acc: 100.0000% Time: 187.2045s\n",
            "#32 Loss: 0.0084 Acc: 99.7330% Time: 193.2983s\n",
            "#33 Loss: 0.0045 Acc: 100.0000% Time: 198.9233s\n",
            "#34 Loss: 0.0197 Acc: 99.7330% Time: 204.6466s\n",
            "#35 Loss: 0.0065 Acc: 100.0000% Time: 210.5171s\n",
            "#36 Loss: 0.0091 Acc: 99.5995% Time: 216.5298s\n",
            "#37 Loss: 0.0030 Acc: 100.0000% Time: 222.3186s\n",
            "#38 Loss: 0.0046 Acc: 99.8665% Time: 228.1199s\n",
            "#39 Loss: 0.0027 Acc: 100.0000% Time: 233.8361s\n",
            "#40 Loss: 0.0090 Acc: 99.7330% Time: 239.7091s\n",
            "#41 Loss: 0.0148 Acc: 99.4660% Time: 245.5560s\n",
            "#42 Loss: 0.0049 Acc: 99.8665% Time: 251.3654s\n",
            "#43 Loss: 0.0021 Acc: 100.0000% Time: 257.0528s\n",
            "#44 Loss: 0.0019 Acc: 100.0000% Time: 262.9869s\n",
            "#45 Loss: 0.0026 Acc: 99.8665% Time: 268.5459s\n",
            "#46 Loss: 0.0050 Acc: 99.8665% Time: 274.1140s\n",
            "#47 Loss: 0.0105 Acc: 99.5995% Time: 280.1098s\n",
            "#48 Loss: 0.0024 Acc: 100.0000% Time: 285.8650s\n",
            "#49 Loss: 0.0024 Acc: 100.0000% Time: 291.4364s\n",
            "#50 Loss: 0.0016 Acc: 100.0000% Time: 297.2403s\n",
            "#51 Loss: 0.0043 Acc: 99.8665% Time: 303.2399s\n",
            "#52 Loss: 0.0056 Acc: 99.5995% Time: 308.9818s\n",
            "#53 Loss: 0.0021 Acc: 100.0000% Time: 314.6879s\n",
            "#54 Loss: 0.0018 Acc: 100.0000% Time: 320.5923s\n",
            "#55 Loss: 0.0083 Acc: 99.7330% Time: 326.2213s\n",
            "#56 Loss: 0.0179 Acc: 99.4660% Time: 331.8928s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#57 Loss: 0.0077 Acc: 99.7330% Time: 337.6022s\n",
            "#58 Loss: 0.0045 Acc: 99.8665% Time: 343.5396s\n",
            "#59 Loss: 0.0067 Acc: 99.7330% Time: 349.5651s\n",
            "#60 Loss: 0.0199 Acc: 99.3324% Time: 355.3036s\n",
            "#61 Loss: 0.0043 Acc: 100.0000% Time: 361.1434s\n",
            "#62 Loss: 0.0019 Acc: 100.0000% Time: 366.9815s\n",
            "#63 Loss: 0.0020 Acc: 100.0000% Time: 372.7931s\n",
            "#64 Loss: 0.0015 Acc: 100.0000% Time: 378.8665s\n",
            "#65 Loss: 0.0021 Acc: 100.0000% Time: 384.6168s\n",
            "#66 Loss: 0.0012 Acc: 100.0000% Time: 390.2200s\n",
            "#67 Loss: 0.0009 Acc: 100.0000% Time: 396.0092s\n",
            "#68 Loss: 0.0009 Acc: 100.0000% Time: 401.9595s\n",
            "#69 Loss: 0.0070 Acc: 99.7330% Time: 407.9967s\n",
            "#70 Loss: 0.0040 Acc: 99.8665% Time: 413.7629s\n",
            "#71 Loss: 0.0012 Acc: 100.0000% Time: 419.7039s\n",
            "#72 Loss: 0.0106 Acc: 99.4660% Time: 425.4848s\n",
            "#73 Loss: 0.0036 Acc: 100.0000% Time: 431.2541s\n",
            "#74 Loss: 0.0174 Acc: 99.5995% Time: 437.2526s\n",
            "#75 Loss: 0.0039 Acc: 99.8665% Time: 443.0523s\n",
            "#76 Loss: 0.0019 Acc: 100.0000% Time: 448.7905s\n",
            "#77 Loss: 0.0076 Acc: 99.7330% Time: 454.8370s\n",
            "#78 Loss: 0.0116 Acc: 99.4660% Time: 460.5398s\n",
            "#79 Loss: 0.0050 Acc: 99.8665% Time: 466.3617s\n",
            "#80 Loss: 0.0019 Acc: 100.0000% Time: 472.4144s\n",
            "#81 Loss: 0.0016 Acc: 100.0000% Time: 478.2140s\n",
            "#82 Loss: 0.0027 Acc: 100.0000% Time: 484.2259s\n",
            "#83 Loss: 0.0006 Acc: 100.0000% Time: 490.3403s\n",
            "#84 Loss: 0.0024 Acc: 100.0000% Time: 496.0555s\n",
            "#85 Loss: 0.0008 Acc: 100.0000% Time: 501.6950s\n",
            "#86 Loss: 0.0091 Acc: 99.5995% Time: 507.3137s\n",
            "#87 Loss: 0.0041 Acc: 99.7330% Time: 512.9733s\n",
            "#88 Loss: 0.0031 Acc: 99.8665% Time: 518.7155s\n",
            "#89 Loss: 0.0023 Acc: 100.0000% Time: 524.4882s\n",
            "#90 Loss: 0.0020 Acc: 100.0000% Time: 530.4199s\n",
            "#91 Loss: 0.0040 Acc: 99.7330% Time: 536.2280s\n",
            "#92 Loss: 0.0011 Acc: 100.0000% Time: 542.1025s\n",
            "#93 Loss: 0.0011 Acc: 100.0000% Time: 548.0560s\n",
            "#94 Loss: 0.0028 Acc: 99.8665% Time: 553.7736s\n",
            "#95 Loss: 0.0014 Acc: 100.0000% Time: 559.4075s\n",
            "#96 Loss: 0.0007 Acc: 100.0000% Time: 565.0966s\n",
            "#97 Loss: 0.0009 Acc: 100.0000% Time: 570.9039s\n",
            "#98 Loss: 0.0080 Acc: 99.8665% Time: 576.8231s\n",
            "#99 Loss: 0.0024 Acc: 100.0000% Time: 582.5769s\n",
            "#100 Loss: 0.0016 Acc: 99.8665% Time: 588.3070s\n",
            "#101 Loss: 0.0005 Acc: 100.0000% Time: 594.1053s\n",
            "#102 Loss: 0.0009 Acc: 100.0000% Time: 599.7591s\n",
            "#103 Loss: 0.0027 Acc: 99.8665% Time: 605.5343s\n",
            "#104 Loss: 0.0014 Acc: 100.0000% Time: 611.3503s\n",
            "#105 Loss: 0.0009 Acc: 100.0000% Time: 617.6305s\n",
            "#106 Loss: 0.0004 Acc: 100.0000% Time: 623.7308s\n",
            "#107 Loss: 0.0007 Acc: 100.0000% Time: 629.5110s\n",
            "#108 Loss: 0.0022 Acc: 99.8665% Time: 635.5895s\n",
            "#109 Loss: 0.0016 Acc: 99.8665% Time: 641.4147s\n",
            "#110 Loss: 0.0101 Acc: 99.5995% Time: 647.0588s\n",
            "#111 Loss: 0.0011 Acc: 100.0000% Time: 653.0995s\n",
            "#112 Loss: 0.0036 Acc: 99.8665% Time: 658.9507s\n",
            "#113 Loss: 0.0008 Acc: 100.0000% Time: 664.8927s\n",
            "#114 Loss: 0.0007 Acc: 100.0000% Time: 670.9264s\n",
            "#115 Loss: 0.0009 Acc: 100.0000% Time: 676.6121s\n",
            "#116 Loss: 0.0005 Acc: 100.0000% Time: 682.3415s\n",
            "#117 Loss: 0.0005 Acc: 100.0000% Time: 688.1101s\n",
            "#118 Loss: 0.0022 Acc: 99.8665% Time: 694.0983s\n",
            "#119 Loss: 0.0007 Acc: 100.0000% Time: 700.0657s\n",
            "#120 Loss: 0.0009 Acc: 100.0000% Time: 705.9128s\n",
            "#121 Loss: 0.0003 Acc: 100.0000% Time: 711.8354s\n",
            "#122 Loss: 0.0012 Acc: 100.0000% Time: 717.6916s\n",
            "#123 Loss: 0.0015 Acc: 100.0000% Time: 723.4778s\n",
            "#124 Loss: 0.0011 Acc: 100.0000% Time: 729.2402s\n",
            "#125 Loss: 0.0006 Acc: 100.0000% Time: 734.9876s\n",
            "#126 Loss: 0.0003 Acc: 100.0000% Time: 740.7805s\n",
            "#127 Loss: 0.0004 Acc: 100.0000% Time: 746.6585s\n",
            "#128 Loss: 0.0004 Acc: 100.0000% Time: 752.6064s\n",
            "#129 Loss: 0.0023 Acc: 99.8665% Time: 758.2592s\n",
            "#130 Loss: 0.0016 Acc: 100.0000% Time: 764.2937s\n",
            "#131 Loss: 0.0004 Acc: 100.0000% Time: 770.3337s\n",
            "#132 Loss: 0.0003 Acc: 100.0000% Time: 776.4830s\n",
            "#133 Loss: 0.0002 Acc: 100.0000% Time: 782.5740s\n",
            "#134 Loss: 0.0009 Acc: 100.0000% Time: 788.2230s\n",
            "#135 Loss: 0.0009 Acc: 100.0000% Time: 794.1206s\n",
            "#136 Loss: 0.0098 Acc: 99.8665% Time: 800.1126s\n",
            "#137 Loss: 0.0009 Acc: 100.0000% Time: 806.2413s\n",
            "#138 Loss: 0.0012 Acc: 100.0000% Time: 811.9503s\n",
            "#139 Loss: 0.0004 Acc: 100.0000% Time: 817.9945s\n",
            "#140 Loss: 0.0004 Acc: 100.0000% Time: 823.9446s\n",
            "#141 Loss: 0.0019 Acc: 100.0000% Time: 829.7921s\n",
            "#142 Loss: 0.0005 Acc: 100.0000% Time: 835.7837s\n",
            "#143 Loss: 0.0006 Acc: 100.0000% Time: 841.8123s\n",
            "#144 Loss: 0.0005 Acc: 100.0000% Time: 847.7658s\n",
            "#145 Loss: 0.0005 Acc: 100.0000% Time: 853.8856s\n",
            "#146 Loss: 0.0345 Acc: 99.5995% Time: 859.9159s\n",
            "#147 Loss: 0.0061 Acc: 99.8665% Time: 865.7930s\n",
            "#148 Loss: 0.0023 Acc: 100.0000% Time: 871.9612s\n",
            "#149 Loss: 0.0009 Acc: 100.0000% Time: 877.9628s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2934 Acc: 91.1290% Time: 1.0853s\n",
            "현재 Best는 epoch : 150 / iteration : 2번째 / Loss : 0.2933852207833326 \n",
            "-------------현재 set_9 3번째 학습-------------\n",
            "#0 Loss: 0.4430 Acc: 78.5047% Time: 6.0236s\n",
            "#1 Loss: 0.2890 Acc: 87.5835% Time: 12.1703s\n",
            "#2 Loss: 0.2213 Acc: 90.1202% Time: 17.8579s\n",
            "#3 Loss: 0.2850 Acc: 88.3845% Time: 24.1902s\n",
            "#4 Loss: 0.1619 Acc: 93.4579% Time: 29.9716s\n",
            "#5 Loss: 0.1089 Acc: 95.5941% Time: 35.9722s\n",
            "#6 Loss: 0.1275 Acc: 95.5941% Time: 42.3085s\n",
            "#7 Loss: 0.0766 Acc: 97.0628% Time: 48.3783s\n",
            "#8 Loss: 0.0849 Acc: 97.0628% Time: 54.4088s\n",
            "#9 Loss: 0.0630 Acc: 97.7303% Time: 60.3417s\n",
            "#10 Loss: 0.0647 Acc: 97.4633% Time: 66.1628s\n",
            "#11 Loss: 0.0519 Acc: 98.1308% Time: 71.9062s\n",
            "#12 Loss: 0.0394 Acc: 98.3979% Time: 77.6811s\n",
            "#13 Loss: 0.0560 Acc: 98.2644% Time: 83.7357s\n",
            "#14 Loss: 0.0327 Acc: 98.7984% Time: 89.6793s\n",
            "#15 Loss: 0.0328 Acc: 98.5314% Time: 95.9394s\n",
            "#16 Loss: 0.0237 Acc: 99.1989% Time: 101.9808s\n",
            "#17 Loss: 0.0184 Acc: 99.4660% Time: 107.7933s\n",
            "#18 Loss: 0.0313 Acc: 98.6649% Time: 113.8398s\n",
            "#19 Loss: 0.0234 Acc: 99.3324% Time: 119.8289s\n",
            "#20 Loss: 0.0178 Acc: 99.3324% Time: 125.9038s\n",
            "#21 Loss: 0.0245 Acc: 99.1989% Time: 132.0371s\n",
            "#22 Loss: 0.0104 Acc: 99.7330% Time: 137.9389s\n",
            "#23 Loss: 0.0264 Acc: 98.6649% Time: 143.7047s\n",
            "#24 Loss: 0.0243 Acc: 98.9319% Time: 150.1079s\n",
            "#25 Loss: 0.0151 Acc: 99.5995% Time: 156.1294s\n",
            "#26 Loss: 0.0119 Acc: 99.5995% Time: 161.8823s\n",
            "#27 Loss: 0.0117 Acc: 99.4660% Time: 167.6931s\n",
            "#28 Loss: 0.0075 Acc: 99.8665% Time: 173.5866s\n",
            "#29 Loss: 0.0096 Acc: 99.8665% Time: 179.6696s\n",
            "#30 Loss: 0.0115 Acc: 99.4660% Time: 185.5680s\n",
            "#31 Loss: 0.0220 Acc: 99.3324% Time: 191.5275s\n",
            "#32 Loss: 0.0125 Acc: 99.8665% Time: 197.5496s\n",
            "#33 Loss: 0.0273 Acc: 99.1989% Time: 203.3146s\n",
            "#34 Loss: 0.0341 Acc: 98.7984% Time: 209.2184s\n",
            "#35 Loss: 0.0132 Acc: 99.4660% Time: 215.0490s\n",
            "#36 Loss: 0.0058 Acc: 100.0000% Time: 221.0489s\n",
            "#37 Loss: 0.0059 Acc: 99.8665% Time: 226.7766s\n",
            "#38 Loss: 0.0141 Acc: 99.4660% Time: 232.9309s\n",
            "#39 Loss: 0.0094 Acc: 99.7330% Time: 238.7950s\n",
            "#40 Loss: 0.0058 Acc: 99.7330% Time: 244.6857s\n",
            "#41 Loss: 0.0074 Acc: 99.7330% Time: 250.4925s\n",
            "#42 Loss: 0.0028 Acc: 100.0000% Time: 256.4915s\n",
            "#43 Loss: 0.0043 Acc: 99.8665% Time: 262.4921s\n",
            "#44 Loss: 0.0032 Acc: 99.8665% Time: 268.3523s\n",
            "#45 Loss: 0.0017 Acc: 100.0000% Time: 274.3325s\n",
            "#46 Loss: 0.0026 Acc: 99.8665% Time: 280.2140s\n",
            "#47 Loss: 0.0074 Acc: 99.5995% Time: 285.8447s\n",
            "#48 Loss: 0.0084 Acc: 99.5995% Time: 291.7897s\n",
            "#49 Loss: 0.0023 Acc: 100.0000% Time: 297.5095s\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2076 Acc: 94.3548% Time: 1.0127s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "현재 Best는 epoch : 50 / iteration : 3번째 / Loss : 0.20758715865574026 \n",
            "-------------현재 set_9 4번째 학습-------------\n",
            "#0 Loss: 0.4631 Acc: 75.7009% Time: 5.9129s\n",
            "#1 Loss: 0.3087 Acc: 87.5835% Time: 12.0145s\n",
            "#2 Loss: 0.1853 Acc: 93.1909% Time: 17.7806s\n",
            "#3 Loss: 0.2013 Acc: 92.1228% Time: 23.5736s\n",
            "#4 Loss: 0.1379 Acc: 95.3271% Time: 29.2426s\n",
            "#5 Loss: 0.1427 Acc: 94.7931% Time: 35.2852s\n",
            "#6 Loss: 0.0828 Acc: 96.9292% Time: 41.3489s\n",
            "#7 Loss: 0.1345 Acc: 93.8585% Time: 47.3088s\n",
            "#8 Loss: 0.0807 Acc: 96.5287% Time: 53.1952s\n",
            "#9 Loss: 0.0473 Acc: 98.6649% Time: 58.9849s\n",
            "#10 Loss: 0.0605 Acc: 97.8638% Time: 64.7450s\n",
            "#11 Loss: 0.0776 Acc: 97.0628% Time: 70.6223s\n",
            "#12 Loss: 0.0565 Acc: 98.1308% Time: 76.6015s\n",
            "#13 Loss: 0.0362 Acc: 98.6649% Time: 82.5840s\n",
            "#14 Loss: 0.0531 Acc: 98.6649% Time: 88.2523s\n",
            "#15 Loss: 0.0345 Acc: 98.7984% Time: 94.3671s\n",
            "#16 Loss: 0.0273 Acc: 99.0654% Time: 100.1369s\n",
            "#17 Loss: 0.0240 Acc: 99.1989% Time: 106.0454s\n",
            "#18 Loss: 0.0324 Acc: 98.7984% Time: 111.9114s\n",
            "#19 Loss: 0.0178 Acc: 99.4660% Time: 117.9265s\n",
            "#20 Loss: 0.0251 Acc: 98.9319% Time: 123.8893s\n",
            "#21 Loss: 0.0264 Acc: 98.7984% Time: 129.8360s\n",
            "#22 Loss: 0.0317 Acc: 98.9319% Time: 135.7675s\n",
            "#23 Loss: 0.0072 Acc: 100.0000% Time: 141.7293s\n",
            "#24 Loss: 0.0663 Acc: 97.9973% Time: 147.4945s\n",
            "#25 Loss: 0.0276 Acc: 99.0654% Time: 153.1955s\n",
            "#26 Loss: 0.0273 Acc: 98.9319% Time: 159.2300s\n",
            "#27 Loss: 0.0211 Acc: 99.1989% Time: 164.9454s\n",
            "#28 Loss: 0.0047 Acc: 100.0000% Time: 170.8684s\n",
            "#29 Loss: 0.0267 Acc: 98.9319% Time: 176.4926s\n",
            "#30 Loss: 0.0093 Acc: 99.8665% Time: 182.3148s\n",
            "#31 Loss: 0.0044 Acc: 100.0000% Time: 188.3995s\n",
            "#32 Loss: 0.0054 Acc: 99.8665% Time: 194.2961s\n",
            "#33 Loss: 0.0102 Acc: 99.5995% Time: 200.1887s\n",
            "#34 Loss: 0.0828 Acc: 97.8638% Time: 206.2601s\n",
            "#35 Loss: 0.0259 Acc: 99.1989% Time: 212.1785s\n",
            "#36 Loss: 0.0093 Acc: 99.7330% Time: 218.0767s\n",
            "#37 Loss: 0.0064 Acc: 99.7330% Time: 224.0835s\n",
            "#38 Loss: 0.0076 Acc: 99.8665% Time: 229.8094s\n",
            "#39 Loss: 0.0054 Acc: 100.0000% Time: 235.5522s\n",
            "#40 Loss: 0.0105 Acc: 99.5995% Time: 241.7256s\n",
            "#41 Loss: 0.0081 Acc: 99.7330% Time: 247.6439s\n",
            "#42 Loss: 0.0070 Acc: 99.8665% Time: 253.5625s\n",
            "#43 Loss: 0.0055 Acc: 99.8665% Time: 259.4522s\n",
            "#44 Loss: 0.0061 Acc: 99.8665% Time: 265.3263s\n",
            "#45 Loss: 0.0085 Acc: 99.7330% Time: 271.2265s\n",
            "#46 Loss: 0.0038 Acc: 100.0000% Time: 276.9366s\n",
            "#47 Loss: 0.0037 Acc: 100.0000% Time: 282.9334s\n",
            "#48 Loss: 0.0089 Acc: 99.7330% Time: 288.7526s\n",
            "#49 Loss: 0.0130 Acc: 99.7330% Time: 295.0906s\n",
            "#50 Loss: 0.0055 Acc: 99.8665% Time: 301.1433s\n",
            "#51 Loss: 0.0063 Acc: 99.7330% Time: 306.8998s\n",
            "#52 Loss: 0.0035 Acc: 99.8665% Time: 312.8289s\n",
            "#53 Loss: 0.0059 Acc: 99.8665% Time: 318.5624s\n",
            "#54 Loss: 0.0035 Acc: 99.8665% Time: 324.5917s\n",
            "#55 Loss: 0.0018 Acc: 100.0000% Time: 330.4811s\n",
            "#56 Loss: 0.0026 Acc: 100.0000% Time: 336.3249s\n",
            "#57 Loss: 0.0058 Acc: 99.8665% Time: 342.0606s\n",
            "#58 Loss: 0.0020 Acc: 100.0000% Time: 347.9100s\n",
            "#59 Loss: 0.0043 Acc: 99.8665% Time: 354.0688s\n",
            "#60 Loss: 0.0037 Acc: 99.8665% Time: 359.8146s\n",
            "#61 Loss: 0.0022 Acc: 100.0000% Time: 365.5603s\n",
            "#62 Loss: 0.0022 Acc: 100.0000% Time: 371.6249s\n",
            "#63 Loss: 0.0015 Acc: 100.0000% Time: 377.3652s\n",
            "#64 Loss: 0.0008 Acc: 100.0000% Time: 383.1076s\n",
            "#65 Loss: 0.0012 Acc: 100.0000% Time: 388.9828s\n",
            "#66 Loss: 0.0009 Acc: 100.0000% Time: 394.6417s\n",
            "#67 Loss: 0.0006 Acc: 100.0000% Time: 400.7355s\n",
            "#68 Loss: 0.0161 Acc: 99.3324% Time: 406.7955s\n",
            "#69 Loss: 0.0061 Acc: 99.7330% Time: 412.9845s\n",
            "#70 Loss: 0.0011 Acc: 100.0000% Time: 418.6987s\n",
            "#71 Loss: 0.0040 Acc: 99.8665% Time: 424.3725s\n",
            "#72 Loss: 0.0016 Acc: 100.0000% Time: 429.9757s\n",
            "#73 Loss: 0.0024 Acc: 100.0000% Time: 435.7289s\n",
            "#74 Loss: 0.0005 Acc: 100.0000% Time: 441.4335s\n",
            "#75 Loss: 0.0015 Acc: 100.0000% Time: 447.4431s\n",
            "#76 Loss: 0.0008 Acc: 100.0000% Time: 453.0875s\n",
            "#77 Loss: 0.0010 Acc: 100.0000% Time: 458.7691s\n",
            "#78 Loss: 0.0008 Acc: 100.0000% Time: 464.4423s\n",
            "#79 Loss: 0.0031 Acc: 99.8665% Time: 470.0982s\n",
            "#80 Loss: 0.0404 Acc: 98.7984% Time: 475.6495s\n",
            "#81 Loss: 0.0088 Acc: 99.5995% Time: 481.4473s\n",
            "#82 Loss: 0.0089 Acc: 99.4660% Time: 486.9912s\n",
            "#83 Loss: 0.0046 Acc: 100.0000% Time: 492.7782s\n",
            "#84 Loss: 0.0039 Acc: 100.0000% Time: 498.7000s\n",
            "#85 Loss: 0.0060 Acc: 99.8665% Time: 504.5420s\n",
            "#86 Loss: 0.0017 Acc: 100.0000% Time: 510.1114s\n",
            "#87 Loss: 0.0026 Acc: 100.0000% Time: 515.9509s\n",
            "#88 Loss: 0.0009 Acc: 100.0000% Time: 521.8373s\n",
            "#89 Loss: 0.0005 Acc: 100.0000% Time: 527.7875s\n",
            "#90 Loss: 0.0016 Acc: 100.0000% Time: 533.5050s\n",
            "#91 Loss: 0.0019 Acc: 99.8665% Time: 539.3020s\n",
            "#92 Loss: 0.0010 Acc: 100.0000% Time: 545.2919s\n",
            "#93 Loss: 0.0004 Acc: 100.0000% Time: 551.0168s\n",
            "#94 Loss: 0.0021 Acc: 100.0000% Time: 556.7447s\n",
            "#95 Loss: 0.0020 Acc: 100.0000% Time: 562.2436s\n",
            "#96 Loss: 0.0006 Acc: 100.0000% Time: 567.8340s\n",
            "#97 Loss: 0.0018 Acc: 100.0000% Time: 573.7639s\n",
            "#98 Loss: 0.0010 Acc: 100.0000% Time: 579.5488s\n",
            "#99 Loss: 0.0004 Acc: 100.0000% Time: 585.5372s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2067 Acc: 93.9516% Time: 1.0347s\n",
            "현재 Best는 epoch : 100 / iteration : 4번째 / Loss : 0.20669178503287125 \n",
            "-------------현재 set_9 5번째 학습-------------\n",
            "#0 Loss: 0.5008 Acc: 76.2350% Time: 5.5909s\n",
            "#1 Loss: 0.2906 Acc: 87.3164% Time: 11.3986s\n",
            "#2 Loss: 0.2217 Acc: 90.6542% Time: 17.3320s\n",
            "#3 Loss: 0.2029 Acc: 91.4553% Time: 23.3633s\n",
            "#4 Loss: 0.1229 Acc: 95.0601% Time: 29.2106s\n",
            "#5 Loss: 0.0949 Acc: 96.1282% Time: 35.1647s\n",
            "#6 Loss: 0.1181 Acc: 95.4606% Time: 40.9866s\n",
            "#7 Loss: 0.0992 Acc: 96.3952% Time: 46.6736s\n",
            "#8 Loss: 0.0430 Acc: 98.6649% Time: 52.7341s\n",
            "#9 Loss: 0.0738 Acc: 97.9973% Time: 58.3033s\n",
            "#10 Loss: 0.0714 Acc: 96.7957% Time: 64.6173s\n",
            "#11 Loss: 0.0285 Acc: 99.0654% Time: 70.3389s\n",
            "#12 Loss: 0.0397 Acc: 98.2644% Time: 76.1149s\n",
            "#13 Loss: 0.0408 Acc: 98.7984% Time: 81.9346s\n",
            "#14 Loss: 0.0558 Acc: 98.1308% Time: 87.5512s\n",
            "#15 Loss: 0.0485 Acc: 98.3979% Time: 93.5236s\n",
            "#16 Loss: 0.0462 Acc: 97.9973% Time: 99.4106s\n",
            "#17 Loss: 0.0346 Acc: 99.0654% Time: 105.2271s\n",
            "#18 Loss: 0.0422 Acc: 98.5314% Time: 110.9589s\n",
            "#19 Loss: 0.0583 Acc: 97.8638% Time: 116.8137s\n",
            "#20 Loss: 0.0353 Acc: 98.9319% Time: 122.8471s\n",
            "#21 Loss: 0.0216 Acc: 99.5995% Time: 128.6221s\n",
            "#22 Loss: 0.0179 Acc: 99.7330% Time: 134.5038s\n",
            "#23 Loss: 0.0154 Acc: 99.4660% Time: 140.2499s\n",
            "#24 Loss: 0.0247 Acc: 99.1989% Time: 146.0357s\n",
            "#25 Loss: 0.0281 Acc: 99.0654% Time: 152.0319s\n",
            "#26 Loss: 0.0117 Acc: 99.4660% Time: 158.0956s\n",
            "#27 Loss: 0.0148 Acc: 99.7330% Time: 164.0299s\n",
            "#28 Loss: 0.0255 Acc: 99.0654% Time: 169.7062s\n",
            "#29 Loss: 0.0064 Acc: 99.8665% Time: 175.6033s\n",
            "#30 Loss: 0.0054 Acc: 99.7330% Time: 181.5812s\n",
            "#31 Loss: 0.0082 Acc: 99.8665% Time: 187.2976s\n",
            "#32 Loss: 0.0063 Acc: 99.7330% Time: 192.8344s\n",
            "#33 Loss: 0.0074 Acc: 99.7330% Time: 198.4404s\n",
            "#34 Loss: 0.0099 Acc: 99.7330% Time: 204.2983s\n",
            "#35 Loss: 0.0333 Acc: 98.7984% Time: 210.4900s\n",
            "#36 Loss: 0.0193 Acc: 99.4660% Time: 216.5642s\n",
            "#37 Loss: 0.0125 Acc: 99.7330% Time: 222.6341s\n",
            "#38 Loss: 0.0038 Acc: 100.0000% Time: 228.5507s\n",
            "#39 Loss: 0.0041 Acc: 100.0000% Time: 234.5960s\n",
            "#40 Loss: 0.0055 Acc: 99.8665% Time: 240.3668s\n",
            "#41 Loss: 0.0273 Acc: 99.0654% Time: 246.4830s\n",
            "#42 Loss: 0.0145 Acc: 99.1989% Time: 252.4994s\n",
            "#43 Loss: 0.0066 Acc: 99.8665% Time: 258.1975s\n",
            "#44 Loss: 0.0039 Acc: 100.0000% Time: 263.7704s\n",
            "#45 Loss: 0.0039 Acc: 100.0000% Time: 269.6584s\n",
            "#46 Loss: 0.0033 Acc: 100.0000% Time: 275.6490s\n",
            "#47 Loss: 0.0044 Acc: 99.8665% Time: 281.8930s\n",
            "#48 Loss: 0.0045 Acc: 99.8665% Time: 287.8240s\n",
            "#49 Loss: 0.0028 Acc: 99.8665% Time: 293.8342s\n",
            "#50 Loss: 0.0015 Acc: 100.0000% Time: 299.6636s\n",
            "#51 Loss: 0.0017 Acc: 100.0000% Time: 305.5216s\n",
            "#52 Loss: 0.0008 Acc: 100.0000% Time: 311.5414s\n",
            "#53 Loss: 0.0016 Acc: 100.0000% Time: 317.2730s\n",
            "#54 Loss: 0.0005 Acc: 100.0000% Time: 323.1401s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#55 Loss: 0.0008 Acc: 100.0000% Time: 328.9190s\n",
            "#56 Loss: 0.0010 Acc: 100.0000% Time: 334.8029s\n",
            "#57 Loss: 0.0009 Acc: 100.0000% Time: 340.7293s\n",
            "#58 Loss: 0.0014 Acc: 100.0000% Time: 346.6976s\n",
            "#59 Loss: 0.0006 Acc: 100.0000% Time: 352.4358s\n",
            "#60 Loss: 0.0010 Acc: 100.0000% Time: 358.4587s\n",
            "#61 Loss: 0.0012 Acc: 100.0000% Time: 364.5831s\n",
            "#62 Loss: 0.0037 Acc: 99.8665% Time: 370.6761s\n",
            "#63 Loss: 0.0129 Acc: 99.4660% Time: 376.7686s\n",
            "#64 Loss: 0.0016 Acc: 100.0000% Time: 382.7151s\n",
            "#65 Loss: 0.0011 Acc: 100.0000% Time: 388.7287s\n",
            "#66 Loss: 0.0020 Acc: 99.8665% Time: 394.6817s\n",
            "#67 Loss: 0.0031 Acc: 100.0000% Time: 400.4813s\n",
            "#68 Loss: 0.0008 Acc: 100.0000% Time: 406.3282s\n",
            "#69 Loss: 0.0020 Acc: 100.0000% Time: 412.0202s\n",
            "#70 Loss: 0.0114 Acc: 99.7330% Time: 417.8405s\n",
            "#71 Loss: 0.0020 Acc: 100.0000% Time: 423.6126s\n",
            "#72 Loss: 0.0011 Acc: 100.0000% Time: 429.3922s\n",
            "#73 Loss: 0.0005 Acc: 100.0000% Time: 435.4234s\n",
            "#74 Loss: 0.0011 Acc: 100.0000% Time: 441.0146s\n",
            "#75 Loss: 0.0014 Acc: 100.0000% Time: 446.6473s\n",
            "#76 Loss: 0.0017 Acc: 100.0000% Time: 452.6225s\n",
            "#77 Loss: 0.0006 Acc: 100.0000% Time: 458.4628s\n",
            "#78 Loss: 0.0005 Acc: 100.0000% Time: 464.1500s\n",
            "#79 Loss: 0.0006 Acc: 100.0000% Time: 470.1537s\n",
            "#80 Loss: 0.0022 Acc: 100.0000% Time: 475.8333s\n",
            "#81 Loss: 0.0338 Acc: 98.7984% Time: 481.8192s\n",
            "#82 Loss: 0.0074 Acc: 99.7330% Time: 487.6003s\n",
            "#83 Loss: 0.0052 Acc: 99.5995% Time: 493.3764s\n",
            "#84 Loss: 0.0038 Acc: 100.0000% Time: 499.0243s\n",
            "#85 Loss: 0.0024 Acc: 100.0000% Time: 504.7820s\n",
            "#86 Loss: 0.0021 Acc: 99.8665% Time: 510.2959s\n",
            "#87 Loss: 0.0026 Acc: 100.0000% Time: 515.8187s\n",
            "#88 Loss: 0.0015 Acc: 100.0000% Time: 521.3709s\n",
            "#89 Loss: 0.0034 Acc: 99.7330% Time: 527.0343s\n",
            "#90 Loss: 0.0014 Acc: 100.0000% Time: 532.6946s\n",
            "#91 Loss: 0.0009 Acc: 100.0000% Time: 538.3893s\n",
            "#92 Loss: 0.0018 Acc: 100.0000% Time: 544.2405s\n",
            "#93 Loss: 0.0013 Acc: 100.0000% Time: 549.9251s\n",
            "#94 Loss: 0.0028 Acc: 99.8665% Time: 555.8432s\n",
            "#95 Loss: 0.0007 Acc: 100.0000% Time: 561.5301s\n",
            "#96 Loss: 0.0034 Acc: 99.8665% Time: 567.2560s\n",
            "#97 Loss: 0.0019 Acc: 100.0000% Time: 573.1750s\n",
            "#98 Loss: 0.0011 Acc: 100.0000% Time: 578.9740s\n",
            "#99 Loss: 0.0004 Acc: 100.0000% Time: 584.7993s\n",
            "#100 Loss: 0.0005 Acc: 100.0000% Time: 590.5161s\n",
            "#101 Loss: 0.0071 Acc: 99.7330% Time: 596.3818s\n",
            "#102 Loss: 0.0016 Acc: 100.0000% Time: 602.1361s\n",
            "#103 Loss: 0.0008 Acc: 100.0000% Time: 608.1119s\n",
            "#104 Loss: 0.0006 Acc: 100.0000% Time: 613.9117s\n",
            "#105 Loss: 0.0009 Acc: 100.0000% Time: 619.6534s\n",
            "#106 Loss: 0.0005 Acc: 100.0000% Time: 625.4719s\n",
            "#107 Loss: 0.0045 Acc: 99.7330% Time: 631.2122s\n",
            "#108 Loss: 0.0019 Acc: 100.0000% Time: 636.9291s\n",
            "#109 Loss: 0.0010 Acc: 100.0000% Time: 642.6920s\n",
            "#110 Loss: 0.0005 Acc: 100.0000% Time: 648.6025s\n",
            "#111 Loss: 0.0015 Acc: 100.0000% Time: 654.2225s\n",
            "#112 Loss: 0.0004 Acc: 100.0000% Time: 660.1586s\n",
            "#113 Loss: 0.0010 Acc: 100.0000% Time: 665.9314s\n",
            "#114 Loss: 0.0003 Acc: 100.0000% Time: 671.5938s\n",
            "#115 Loss: 0.0004 Acc: 100.0000% Time: 677.2496s\n",
            "#116 Loss: 0.0007 Acc: 100.0000% Time: 683.1786s\n",
            "#117 Loss: 0.0006 Acc: 100.0000% Time: 689.2256s\n",
            "#118 Loss: 0.0005 Acc: 100.0000% Time: 695.0621s\n",
            "#119 Loss: 0.0003 Acc: 100.0000% Time: 700.7135s\n",
            "#120 Loss: 0.0004 Acc: 100.0000% Time: 706.4214s\n",
            "#121 Loss: 0.0002 Acc: 100.0000% Time: 712.3584s\n",
            "#122 Loss: 0.0002 Acc: 100.0000% Time: 718.0860s\n",
            "#123 Loss: 0.0011 Acc: 100.0000% Time: 723.9135s\n",
            "#124 Loss: 0.0002 Acc: 100.0000% Time: 729.5836s\n",
            "#125 Loss: 0.0006 Acc: 100.0000% Time: 735.5869s\n",
            "#126 Loss: 0.0002 Acc: 100.0000% Time: 741.3261s\n",
            "#127 Loss: 0.0004 Acc: 100.0000% Time: 747.1710s\n",
            "#128 Loss: 0.0014 Acc: 99.8665% Time: 753.1945s\n",
            "#129 Loss: 0.0002 Acc: 100.0000% Time: 759.5330s\n",
            "#130 Loss: 0.0004 Acc: 100.0000% Time: 765.6134s\n",
            "#131 Loss: 0.0003 Acc: 100.0000% Time: 771.3011s\n",
            "#132 Loss: 0.0004 Acc: 100.0000% Time: 776.8228s\n",
            "#133 Loss: 0.0009 Acc: 100.0000% Time: 782.7264s\n",
            "#134 Loss: 0.0005 Acc: 100.0000% Time: 788.7418s\n",
            "#135 Loss: 0.0004 Acc: 100.0000% Time: 794.4463s\n",
            "#136 Loss: 0.0001 Acc: 100.0000% Time: 800.0460s\n",
            "#137 Loss: 0.0002 Acc: 100.0000% Time: 806.0239s\n",
            "#138 Loss: 0.0002 Acc: 100.0000% Time: 811.6658s\n",
            "#139 Loss: 0.0011 Acc: 100.0000% Time: 817.7658s\n",
            "#140 Loss: 0.0010 Acc: 100.0000% Time: 823.5622s\n",
            "#141 Loss: 0.0004 Acc: 100.0000% Time: 829.4752s\n",
            "#142 Loss: 0.0011 Acc: 100.0000% Time: 835.5356s\n",
            "#143 Loss: 0.0007 Acc: 100.0000% Time: 841.8629s\n",
            "#144 Loss: 0.0005 Acc: 100.0000% Time: 847.9900s\n",
            "#145 Loss: 0.0003 Acc: 100.0000% Time: 853.9649s\n",
            "#146 Loss: 0.0002 Acc: 100.0000% Time: 859.9399s\n",
            "#147 Loss: 0.0002 Acc: 100.0000% Time: 865.7053s\n",
            "#148 Loss: 0.0003 Acc: 100.0000% Time: 871.7250s\n",
            "#149 Loss: 0.0003 Acc: 100.0000% Time: 877.8013s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[Test Phase] Loss: 0.2811 Acc: 94.3548% Time: 1.0711s\n",
            "현재 Best는 epoch : 150 / iteration : 5번째 / Loss : 0.28108396162937305 \n",
            "-------------현재 set_9 6번째 학습-------------\n",
            "#0 Loss: 0.4991 Acc: 76.3685% Time: 5.8332s\n",
            "#1 Loss: 0.2835 Acc: 87.7170% Time: 11.5390s\n",
            "#2 Loss: 0.2139 Acc: 90.7877% Time: 17.2426s\n",
            "#3 Loss: 0.1973 Acc: 93.5915% Time: 23.3345s\n",
            "#4 Loss: 0.1426 Acc: 95.4606% Time: 29.6806s\n",
            "#5 Loss: 0.0771 Acc: 97.7303% Time: 35.4980s\n",
            "#6 Loss: 0.1398 Acc: 94.3925% Time: 41.5640s\n",
            "#7 Loss: 0.0957 Acc: 95.9947% Time: 47.6309s\n",
            "#8 Loss: 0.0445 Acc: 98.3979% Time: 53.4676s\n",
            "#9 Loss: 0.0350 Acc: 98.9319% Time: 59.4705s\n",
            "#10 Loss: 0.0570 Acc: 97.3298% Time: 65.4838s\n",
            "#11 Loss: 0.0787 Acc: 97.9973% Time: 71.4691s\n",
            "#12 Loss: 0.0379 Acc: 98.3979% Time: 77.2424s\n",
            "#13 Loss: 0.0440 Acc: 98.2644% Time: 82.9823s\n",
            "#14 Loss: 0.0352 Acc: 98.6649% Time: 88.8177s\n",
            "#15 Loss: 0.0421 Acc: 98.6649% Time: 94.6570s\n",
            "#16 Loss: 0.0346 Acc: 99.0654% Time: 100.8764s\n",
            "#17 Loss: 0.0251 Acc: 98.9319% Time: 107.0769s\n",
            "#18 Loss: 0.0376 Acc: 98.7984% Time: 112.7361s\n",
            "#19 Loss: 0.0350 Acc: 98.9319% Time: 119.0538s\n",
            "#20 Loss: 0.0162 Acc: 99.4660% Time: 124.8873s\n",
            "#21 Loss: 0.0298 Acc: 98.7984% Time: 130.6072s\n",
            "#22 Loss: 0.0234 Acc: 99.0654% Time: 136.5790s\n",
            "#23 Loss: 0.0262 Acc: 98.9319% Time: 142.5483s\n",
            "#24 Loss: 0.0326 Acc: 99.0654% Time: 148.5889s\n",
            "#25 Loss: 0.0130 Acc: 99.5995% Time: 154.3121s\n",
            "#26 Loss: 0.0381 Acc: 98.5314% Time: 160.1004s\n",
            "#27 Loss: 0.0223 Acc: 99.1989% Time: 166.1364s\n",
            "#28 Loss: 0.0182 Acc: 99.4660% Time: 172.0484s\n",
            "#29 Loss: 0.0102 Acc: 99.5995% Time: 177.9225s\n",
            "#30 Loss: 0.0125 Acc: 99.4660% Time: 183.7108s\n",
            "#31 Loss: 0.0097 Acc: 99.7330% Time: 189.7205s\n",
            "#32 Loss: 0.0091 Acc: 99.8665% Time: 195.6136s\n",
            "#33 Loss: 0.0361 Acc: 98.5314% Time: 201.3573s\n",
            "#34 Loss: 0.0096 Acc: 99.7330% Time: 207.2262s\n",
            "#35 Loss: 0.0172 Acc: 99.0654% Time: 213.0663s\n",
            "#36 Loss: 0.0060 Acc: 99.8665% Time: 219.0338s\n",
            "#37 Loss: 0.0065 Acc: 99.8665% Time: 224.8280s\n",
            "#38 Loss: 0.0085 Acc: 99.7330% Time: 230.8382s\n",
            "#39 Loss: 0.0061 Acc: 100.0000% Time: 236.5247s\n",
            "#40 Loss: 0.0073 Acc: 99.4660% Time: 242.7411s\n",
            "#41 Loss: 0.0084 Acc: 99.8665% Time: 248.6123s\n",
            "#42 Loss: 0.0089 Acc: 99.4660% Time: 254.3508s\n",
            "#43 Loss: 0.0022 Acc: 100.0000% Time: 260.1402s\n",
            "#44 Loss: 0.0218 Acc: 99.3324% Time: 266.2600s\n",
            "#45 Loss: 0.0057 Acc: 99.8665% Time: 272.2332s\n",
            "#46 Loss: 0.0080 Acc: 99.5995% Time: 278.1295s\n",
            "#47 Loss: 0.0049 Acc: 99.8665% Time: 284.2240s\n",
            "#48 Loss: 0.0030 Acc: 99.8665% Time: 290.2700s\n",
            "#49 Loss: 0.0041 Acc: 100.0000% Time: 296.3006s\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2434 Acc: 93.5484% Time: 1.0791s\n",
            "현재 Best는 epoch : 50 / iteration : 6번째 / Loss : 0.24342889199359038 \n",
            "-------------현재 set_9 7번째 학습-------------\n",
            "#0 Loss: 0.4624 Acc: 79.0387% Time: 6.1562s\n",
            "#1 Loss: 0.2891 Acc: 88.2510% Time: 11.8784s\n",
            "#2 Loss: 0.1866 Acc: 92.5234% Time: 17.9352s\n",
            "#3 Loss: 0.1951 Acc: 92.6569% Time: 23.8208s\n",
            "#4 Loss: 0.1725 Acc: 93.5915% Time: 29.6969s\n",
            "#5 Loss: 0.1471 Acc: 94.5260% Time: 35.6027s\n",
            "#6 Loss: 0.1275 Acc: 95.0601% Time: 41.5449s\n",
            "#7 Loss: 0.0945 Acc: 96.3952% Time: 47.4494s\n",
            "#8 Loss: 0.1101 Acc: 96.5287% Time: 53.3059s\n",
            "#9 Loss: 0.1276 Acc: 95.9947% Time: 59.1396s\n",
            "#10 Loss: 0.0507 Acc: 98.3979% Time: 65.2416s\n",
            "#11 Loss: 0.0394 Acc: 98.7984% Time: 71.0229s\n",
            "#12 Loss: 0.0405 Acc: 98.7984% Time: 76.8287s\n",
            "#13 Loss: 0.0277 Acc: 98.7984% Time: 82.8925s\n",
            "#14 Loss: 0.0329 Acc: 99.1989% Time: 88.8899s\n",
            "#15 Loss: 0.0310 Acc: 99.1989% Time: 94.8730s\n",
            "#16 Loss: 0.0409 Acc: 98.7984% Time: 100.9879s\n",
            "#17 Loss: 0.0115 Acc: 99.7330% Time: 106.7514s\n",
            "#18 Loss: 0.0141 Acc: 99.5995% Time: 112.7497s\n",
            "#19 Loss: 0.0201 Acc: 99.1989% Time: 118.7912s\n",
            "#20 Loss: 0.0239 Acc: 99.3324% Time: 124.3176s\n",
            "#21 Loss: 0.0277 Acc: 98.6649% Time: 130.2127s\n",
            "#22 Loss: 0.0131 Acc: 99.7330% Time: 136.0019s\n",
            "#23 Loss: 0.0136 Acc: 99.7330% Time: 141.9961s\n",
            "#24 Loss: 0.0206 Acc: 99.1989% Time: 147.8551s\n",
            "#25 Loss: 0.0171 Acc: 99.3324% Time: 153.6670s\n",
            "#26 Loss: 0.0072 Acc: 100.0000% Time: 159.2718s\n",
            "#27 Loss: 0.0047 Acc: 99.8665% Time: 165.2007s\n",
            "#28 Loss: 0.0067 Acc: 100.0000% Time: 170.9575s\n",
            "#29 Loss: 0.0106 Acc: 99.7330% Time: 177.0736s\n",
            "#30 Loss: 0.0098 Acc: 99.7330% Time: 182.7764s\n",
            "#31 Loss: 0.0050 Acc: 99.8665% Time: 188.7717s\n",
            "#32 Loss: 0.0101 Acc: 99.7330% Time: 194.7879s\n",
            "#33 Loss: 0.0337 Acc: 98.6649% Time: 200.5484s\n",
            "#34 Loss: 0.0135 Acc: 99.1989% Time: 206.2964s\n",
            "#35 Loss: 0.0217 Acc: 99.5995% Time: 211.9766s\n",
            "#36 Loss: 0.0174 Acc: 99.4660% Time: 217.9217s\n",
            "#37 Loss: 0.0350 Acc: 99.1989% Time: 223.7218s\n",
            "#38 Loss: 0.0133 Acc: 99.4660% Time: 229.6001s\n",
            "#39 Loss: 0.0036 Acc: 100.0000% Time: 235.3973s\n",
            "#40 Loss: 0.0446 Acc: 99.0654% Time: 241.3864s\n",
            "#41 Loss: 0.0187 Acc: 99.5995% Time: 247.5758s\n",
            "#42 Loss: 0.0126 Acc: 99.5995% Time: 253.3175s\n",
            "#43 Loss: 0.0080 Acc: 100.0000% Time: 259.0225s\n",
            "#44 Loss: 0.0091 Acc: 99.7330% Time: 264.7897s\n",
            "#45 Loss: 0.0091 Acc: 99.5995% Time: 270.7795s\n",
            "#46 Loss: 0.0059 Acc: 100.0000% Time: 276.5253s\n",
            "#47 Loss: 0.0027 Acc: 100.0000% Time: 282.3181s\n",
            "#48 Loss: 0.0046 Acc: 100.0000% Time: 288.0421s\n",
            "#49 Loss: 0.0032 Acc: 100.0000% Time: 293.9085s\n",
            "#50 Loss: 0.0011 Acc: 100.0000% Time: 299.9145s\n",
            "#51 Loss: 0.0034 Acc: 99.8665% Time: 305.9985s\n",
            "#52 Loss: 0.0148 Acc: 99.4660% Time: 311.7042s\n",
            "#53 Loss: 0.0041 Acc: 100.0000% Time: 317.5820s\n",
            "#54 Loss: 0.0020 Acc: 100.0000% Time: 323.2562s\n",
            "#55 Loss: 0.0127 Acc: 99.7330% Time: 328.9229s\n",
            "#56 Loss: 0.0048 Acc: 99.8665% Time: 334.9617s\n",
            "#57 Loss: 0.0067 Acc: 99.8665% Time: 340.8426s\n",
            "#58 Loss: 0.0042 Acc: 100.0000% Time: 346.6713s\n",
            "#59 Loss: 0.0090 Acc: 99.8665% Time: 352.4890s\n",
            "#60 Loss: 0.0187 Acc: 99.4660% Time: 358.3871s\n",
            "#61 Loss: 0.0023 Acc: 100.0000% Time: 364.4552s\n",
            "#62 Loss: 0.0016 Acc: 100.0000% Time: 370.4527s\n",
            "#63 Loss: 0.0081 Acc: 99.7330% Time: 376.1061s\n",
            "#64 Loss: 0.0109 Acc: 99.7330% Time: 381.9482s\n",
            "#65 Loss: 0.0055 Acc: 99.8665% Time: 387.9122s\n",
            "#66 Loss: 0.0020 Acc: 100.0000% Time: 393.8489s\n",
            "#67 Loss: 0.0045 Acc: 99.8665% Time: 399.5761s\n",
            "#68 Loss: 0.0060 Acc: 99.8665% Time: 405.6879s\n",
            "#69 Loss: 0.0014 Acc: 100.0000% Time: 411.4869s\n",
            "#70 Loss: 0.0028 Acc: 100.0000% Time: 417.4145s\n",
            "#71 Loss: 0.0016 Acc: 100.0000% Time: 423.2353s\n",
            "#72 Loss: 0.0050 Acc: 99.8665% Time: 429.2452s\n",
            "#73 Loss: 0.0086 Acc: 99.8665% Time: 434.9883s\n",
            "#74 Loss: 0.0099 Acc: 99.7330% Time: 441.2065s\n",
            "#75 Loss: 0.0068 Acc: 99.8665% Time: 447.0004s\n",
            "#76 Loss: 0.0050 Acc: 99.8665% Time: 452.8683s\n",
            "#77 Loss: 0.0036 Acc: 100.0000% Time: 458.7103s\n",
            "#78 Loss: 0.0018 Acc: 100.0000% Time: 464.6662s\n",
            "#79 Loss: 0.0014 Acc: 100.0000% Time: 470.4663s\n",
            "#80 Loss: 0.0007 Acc: 100.0000% Time: 476.4867s\n",
            "#81 Loss: 0.0012 Acc: 100.0000% Time: 482.2079s\n",
            "#82 Loss: 0.0013 Acc: 100.0000% Time: 488.3334s\n",
            "#83 Loss: 0.0008 Acc: 100.0000% Time: 493.9502s\n",
            "#84 Loss: 0.0041 Acc: 99.8665% Time: 499.8558s\n",
            "#85 Loss: 0.0009 Acc: 100.0000% Time: 505.6411s\n",
            "#86 Loss: 0.0005 Acc: 100.0000% Time: 511.3842s\n",
            "#87 Loss: 0.0010 Acc: 100.0000% Time: 517.3845s\n",
            "#88 Loss: 0.0004 Acc: 100.0000% Time: 523.3351s\n",
            "#89 Loss: 0.0007 Acc: 100.0000% Time: 529.3056s\n",
            "#90 Loss: 0.0008 Acc: 100.0000% Time: 535.4873s\n",
            "#91 Loss: 0.0017 Acc: 100.0000% Time: 541.3047s\n",
            "#92 Loss: 0.0017 Acc: 99.8665% Time: 547.1052s\n",
            "#93 Loss: 0.0012 Acc: 100.0000% Time: 553.1535s\n",
            "#94 Loss: 0.0005 Acc: 100.0000% Time: 559.1875s\n",
            "#95 Loss: 0.0008 Acc: 100.0000% Time: 565.3227s\n",
            "#96 Loss: 0.0006 Acc: 100.0000% Time: 571.3731s\n",
            "#97 Loss: 0.0006 Acc: 100.0000% Time: 577.4786s\n",
            "#98 Loss: 0.0002 Acc: 100.0000% Time: 583.2474s\n",
            "#99 Loss: 0.0002 Acc: 100.0000% Time: 589.2771s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.1851 Acc: 95.1613% Time: 1.0950s\n",
            "현재 Best는 epoch : 100 / iteration : 7번째 / Loss : 0.18509611624458264 \n",
            "-------------현재 set_9 8번째 학습-------------\n",
            "#0 Loss: 0.5231 Acc: 76.1015% Time: 5.8807s\n",
            "#1 Loss: 0.2566 Acc: 90.1202% Time: 11.6686s\n",
            "#2 Loss: 0.2710 Acc: 88.9186% Time: 17.6268s\n",
            "#3 Loss: 0.1765 Acc: 93.4579% Time: 23.3845s\n",
            "#4 Loss: 0.1276 Acc: 95.0601% Time: 29.1635s\n",
            "#5 Loss: 0.1035 Acc: 96.5287% Time: 35.0069s\n",
            "#6 Loss: 0.1012 Acc: 96.3952% Time: 41.0106s\n",
            "#7 Loss: 0.0718 Acc: 97.3298% Time: 46.9587s\n",
            "#8 Loss: 0.0767 Acc: 97.1963% Time: 52.9191s\n",
            "#9 Loss: 0.0795 Acc: 97.1963% Time: 58.9578s\n",
            "#10 Loss: 0.0584 Acc: 97.9973% Time: 64.6485s\n",
            "#11 Loss: 0.0413 Acc: 98.5314% Time: 70.6230s\n",
            "#12 Loss: 0.0311 Acc: 98.7984% Time: 76.4700s\n",
            "#13 Loss: 0.0355 Acc: 98.9319% Time: 82.5775s\n",
            "#14 Loss: 0.0296 Acc: 99.5995% Time: 88.2432s\n",
            "#15 Loss: 0.0470 Acc: 98.2644% Time: 94.2404s\n",
            "#16 Loss: 0.0341 Acc: 98.7984% Time: 100.0350s\n",
            "#17 Loss: 0.0187 Acc: 99.5995% Time: 105.9787s\n",
            "#18 Loss: 0.0165 Acc: 99.4660% Time: 111.7805s\n",
            "#19 Loss: 0.0277 Acc: 98.7984% Time: 117.6757s\n",
            "#20 Loss: 0.0206 Acc: 99.3324% Time: 123.6263s\n",
            "#21 Loss: 0.0159 Acc: 99.4660% Time: 129.5077s\n",
            "#22 Loss: 0.0359 Acc: 99.0654% Time: 135.4708s\n",
            "#23 Loss: 0.0343 Acc: 98.6649% Time: 141.5737s\n",
            "#24 Loss: 0.0222 Acc: 98.7984% Time: 147.3390s\n",
            "#25 Loss: 0.0137 Acc: 99.8665% Time: 153.1336s\n",
            "#26 Loss: 0.0103 Acc: 99.7330% Time: 159.1070s\n",
            "#27 Loss: 0.0615 Acc: 98.3979% Time: 165.0179s\n",
            "#28 Loss: 0.0206 Acc: 99.4660% Time: 170.7534s\n",
            "#29 Loss: 0.0137 Acc: 99.8665% Time: 176.7675s\n",
            "#30 Loss: 0.0105 Acc: 99.8665% Time: 182.8676s\n",
            "#31 Loss: 0.0115 Acc: 99.8665% Time: 188.9418s\n",
            "#32 Loss: 0.0218 Acc: 99.4660% Time: 194.7922s\n",
            "#33 Loss: 0.0199 Acc: 99.3324% Time: 200.6061s\n",
            "#34 Loss: 0.0092 Acc: 99.7330% Time: 206.3799s\n",
            "#35 Loss: 0.0082 Acc: 99.8665% Time: 212.2152s\n",
            "#36 Loss: 0.0178 Acc: 99.4660% Time: 218.1382s\n",
            "#37 Loss: 0.0116 Acc: 99.7330% Time: 224.1958s\n",
            "#38 Loss: 0.0343 Acc: 99.0654% Time: 230.2894s\n",
            "#39 Loss: 0.0074 Acc: 99.8665% Time: 235.9620s\n",
            "#40 Loss: 0.0082 Acc: 99.7330% Time: 241.6463s\n",
            "#41 Loss: 0.0229 Acc: 99.5995% Time: 247.6604s\n",
            "#42 Loss: 0.0046 Acc: 100.0000% Time: 253.4335s\n",
            "#43 Loss: 0.0014 Acc: 100.0000% Time: 259.4090s\n",
            "#44 Loss: 0.0196 Acc: 99.4660% Time: 265.2604s\n",
            "#45 Loss: 0.0055 Acc: 99.8665% Time: 271.1060s\n",
            "#46 Loss: 0.0060 Acc: 99.8665% Time: 277.0281s\n",
            "#47 Loss: 0.0143 Acc: 99.7330% Time: 282.6596s\n",
            "#48 Loss: 0.0038 Acc: 100.0000% Time: 288.3314s\n",
            "#49 Loss: 0.0084 Acc: 99.7330% Time: 294.3369s\n",
            "#50 Loss: 0.0029 Acc: 100.0000% Time: 300.4435s\n",
            "#51 Loss: 0.0033 Acc: 100.0000% Time: 306.2527s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#52 Loss: 0.0133 Acc: 99.4660% Time: 312.0244s\n",
            "#53 Loss: 0.0064 Acc: 99.7330% Time: 318.0785s\n",
            "#54 Loss: 0.0053 Acc: 99.8665% Time: 323.8759s\n",
            "#55 Loss: 0.0042 Acc: 99.8665% Time: 329.8785s\n",
            "#56 Loss: 0.0053 Acc: 99.8665% Time: 335.7046s\n",
            "#57 Loss: 0.0039 Acc: 99.8665% Time: 341.6317s\n",
            "#58 Loss: 0.0019 Acc: 100.0000% Time: 347.2373s\n",
            "#59 Loss: 0.0093 Acc: 99.8665% Time: 353.1429s\n",
            "#60 Loss: 0.0084 Acc: 99.5995% Time: 358.8557s\n",
            "#61 Loss: 0.0018 Acc: 100.0000% Time: 364.6928s\n",
            "#62 Loss: 0.0018 Acc: 100.0000% Time: 370.3546s\n",
            "#63 Loss: 0.0013 Acc: 100.0000% Time: 376.2552s\n",
            "#64 Loss: 0.0014 Acc: 100.0000% Time: 382.1893s\n",
            "#65 Loss: 0.0014 Acc: 100.0000% Time: 388.1364s\n",
            "#66 Loss: 0.0012 Acc: 100.0000% Time: 394.1049s\n",
            "#67 Loss: 0.0009 Acc: 100.0000% Time: 400.0046s\n",
            "#68 Loss: 0.0072 Acc: 99.8665% Time: 405.8311s\n",
            "#69 Loss: 0.0026 Acc: 100.0000% Time: 411.7634s\n",
            "#70 Loss: 0.0011 Acc: 100.0000% Time: 417.5424s\n",
            "#71 Loss: 0.0006 Acc: 100.0000% Time: 423.4528s\n",
            "#72 Loss: 0.0022 Acc: 99.8665% Time: 429.3655s\n",
            "#73 Loss: 0.0012 Acc: 100.0000% Time: 435.3018s\n",
            "#74 Loss: 0.0008 Acc: 100.0000% Time: 441.2682s\n",
            "#75 Loss: 0.0007 Acc: 100.0000% Time: 447.1874s\n",
            "#76 Loss: 0.0008 Acc: 100.0000% Time: 452.9001s\n",
            "#77 Loss: 0.0061 Acc: 99.7330% Time: 458.8290s\n",
            "#78 Loss: 0.0015 Acc: 100.0000% Time: 464.9251s\n",
            "#79 Loss: 0.0009 Acc: 100.0000% Time: 470.9066s\n",
            "#80 Loss: 0.0005 Acc: 100.0000% Time: 476.8419s\n",
            "#81 Loss: 0.0008 Acc: 100.0000% Time: 482.5833s\n",
            "#82 Loss: 0.0028 Acc: 99.8665% Time: 488.4301s\n",
            "#83 Loss: 0.0015 Acc: 100.0000% Time: 494.3298s\n",
            "#84 Loss: 0.0037 Acc: 99.8665% Time: 500.1038s\n",
            "#85 Loss: 0.0081 Acc: 99.8665% Time: 505.8197s\n",
            "#86 Loss: 0.0024 Acc: 99.8665% Time: 511.7160s\n",
            "#87 Loss: 0.0012 Acc: 100.0000% Time: 517.6499s\n",
            "#88 Loss: 0.0205 Acc: 99.5995% Time: 523.6584s\n",
            "#89 Loss: 0.0110 Acc: 99.7330% Time: 529.5010s\n",
            "#90 Loss: 0.0014 Acc: 100.0000% Time: 535.2940s\n",
            "#91 Loss: 0.0017 Acc: 100.0000% Time: 541.2320s\n",
            "#92 Loss: 0.0019 Acc: 99.8665% Time: 547.0363s\n",
            "#93 Loss: 0.0012 Acc: 100.0000% Time: 552.9333s\n",
            "#94 Loss: 0.0006 Acc: 100.0000% Time: 558.8435s\n",
            "#95 Loss: 0.0056 Acc: 99.7330% Time: 564.6324s\n",
            "#96 Loss: 0.0011 Acc: 100.0000% Time: 570.5251s\n",
            "#97 Loss: 0.0020 Acc: 100.0000% Time: 576.3015s\n",
            "#98 Loss: 0.0007 Acc: 100.0000% Time: 582.0917s\n",
            "#99 Loss: 0.0016 Acc: 100.0000% Time: 588.1129s\n",
            "#100 Loss: 0.0010 Acc: 100.0000% Time: 594.2346s\n",
            "#101 Loss: 0.0011 Acc: 100.0000% Time: 600.2703s\n",
            "#102 Loss: 0.0004 Acc: 100.0000% Time: 606.1388s\n",
            "#103 Loss: 0.0062 Acc: 99.7330% Time: 611.9843s\n",
            "#104 Loss: 0.0011 Acc: 100.0000% Time: 617.8210s\n",
            "#105 Loss: 0.0011 Acc: 100.0000% Time: 623.3555s\n",
            "#106 Loss: 0.0006 Acc: 100.0000% Time: 629.1496s\n",
            "#107 Loss: 0.0004 Acc: 100.0000% Time: 635.0784s\n",
            "#108 Loss: 0.0003 Acc: 100.0000% Time: 640.8167s\n",
            "#109 Loss: 0.0004 Acc: 100.0000% Time: 646.5901s\n",
            "#110 Loss: 0.0006 Acc: 100.0000% Time: 652.5032s\n",
            "#111 Loss: 0.0006 Acc: 100.0000% Time: 658.2598s\n",
            "#112 Loss: 0.0004 Acc: 100.0000% Time: 664.0296s\n",
            "#113 Loss: 0.0007 Acc: 100.0000% Time: 670.1640s\n",
            "#114 Loss: 0.0003 Acc: 100.0000% Time: 676.0748s\n",
            "#115 Loss: 0.0007 Acc: 100.0000% Time: 681.8684s\n",
            "#116 Loss: 0.0006 Acc: 100.0000% Time: 687.5830s\n",
            "#117 Loss: 0.0002 Acc: 100.0000% Time: 693.5294s\n",
            "#118 Loss: 0.0003 Acc: 100.0000% Time: 699.5128s\n",
            "#119 Loss: 0.0003 Acc: 100.0000% Time: 705.3682s\n",
            "#120 Loss: 0.0003 Acc: 100.0000% Time: 711.3641s\n",
            "#121 Loss: 0.0002 Acc: 100.0000% Time: 717.0321s\n",
            "#122 Loss: 0.0004 Acc: 100.0000% Time: 722.8403s\n",
            "#123 Loss: 0.0002 Acc: 100.0000% Time: 728.5522s\n",
            "#124 Loss: 0.0006 Acc: 100.0000% Time: 734.7545s\n",
            "#125 Loss: 0.0025 Acc: 99.8665% Time: 740.4695s\n",
            "#126 Loss: 0.0012 Acc: 100.0000% Time: 746.2152s\n",
            "#127 Loss: 0.0002 Acc: 100.0000% Time: 752.0309s\n",
            "#128 Loss: 0.0006 Acc: 100.0000% Time: 758.0901s\n",
            "#129 Loss: 0.0006 Acc: 100.0000% Time: 763.8431s\n",
            "#130 Loss: 0.0005 Acc: 100.0000% Time: 769.6651s\n",
            "#131 Loss: 0.0004 Acc: 100.0000% Time: 775.5959s\n",
            "#132 Loss: 0.0006 Acc: 100.0000% Time: 781.4456s\n",
            "#133 Loss: 0.0003 Acc: 100.0000% Time: 787.3357s\n",
            "#134 Loss: 0.0002 Acc: 100.0000% Time: 793.1803s\n",
            "#135 Loss: 0.0004 Acc: 100.0000% Time: 799.3133s\n",
            "#136 Loss: 0.0005 Acc: 100.0000% Time: 805.2645s\n",
            "#137 Loss: 0.0004 Acc: 100.0000% Time: 811.1305s\n",
            "#138 Loss: 0.0021 Acc: 99.8665% Time: 817.4457s\n",
            "#139 Loss: 0.0007 Acc: 100.0000% Time: 823.2851s\n",
            "#140 Loss: 0.0004 Acc: 100.0000% Time: 829.1170s\n",
            "#141 Loss: 0.0002 Acc: 100.0000% Time: 835.0615s\n",
            "#142 Loss: 0.0133 Acc: 99.3324% Time: 841.1355s\n",
            "#143 Loss: 0.0011 Acc: 100.0000% Time: 847.3033s\n",
            "#144 Loss: 0.0005 Acc: 100.0000% Time: 853.1334s\n",
            "#145 Loss: 0.0019 Acc: 99.8665% Time: 859.0934s\n",
            "#146 Loss: 0.0005 Acc: 100.0000% Time: 864.8229s\n",
            "#147 Loss: 0.0008 Acc: 100.0000% Time: 870.6679s\n",
            "#148 Loss: 0.0004 Acc: 100.0000% Time: 876.6649s\n",
            "#149 Loss: 0.0056 Acc: 99.7330% Time: 882.3695s\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[Test Phase] Loss: 0.2072 Acc: 91.9355% Time: 1.0528s\n",
            "현재 Best는 epoch : 150 / iteration : 8번째 / Loss : 0.20722513832820808 \n",
            "-------------현재 set_9 9번째 학습-------------\n",
            "#0 Loss: 0.5308 Acc: 74.7664% Time: 5.8528s\n",
            "#1 Loss: 0.2851 Acc: 88.6515% Time: 11.7183s\n",
            "#2 Loss: 0.1998 Acc: 91.4553% Time: 17.5927s\n",
            "#3 Loss: 0.1640 Acc: 93.1909% Time: 23.7598s\n",
            "#4 Loss: 0.1962 Acc: 92.5234% Time: 29.8146s\n",
            "#5 Loss: 0.1366 Acc: 94.7931% Time: 35.8618s\n",
            "#6 Loss: 0.1125 Acc: 95.8612% Time: 41.5173s\n",
            "#7 Loss: 0.0609 Acc: 97.5968% Time: 47.3113s\n",
            "#8 Loss: 0.0819 Acc: 96.6622% Time: 53.4335s\n",
            "#9 Loss: 0.0668 Acc: 97.4633% Time: 59.6444s\n",
            "#10 Loss: 0.0549 Acc: 98.1308% Time: 65.7226s\n",
            "#11 Loss: 0.0557 Acc: 97.8638% Time: 71.4655s\n",
            "#12 Loss: 0.0351 Acc: 99.1989% Time: 77.5518s\n",
            "#13 Loss: 0.0264 Acc: 99.0654% Time: 83.4315s\n",
            "#14 Loss: 0.0474 Acc: 98.2644% Time: 89.5210s\n",
            "#15 Loss: 0.0329 Acc: 99.0654% Time: 95.3216s\n",
            "#16 Loss: 0.0291 Acc: 99.1989% Time: 101.3319s\n",
            "#17 Loss: 0.0577 Acc: 97.7303% Time: 107.2767s\n",
            "#18 Loss: 0.0159 Acc: 99.4660% Time: 113.3621s\n",
            "#19 Loss: 0.0234 Acc: 99.3324% Time: 119.2747s\n",
            "#20 Loss: 0.0192 Acc: 99.4660% Time: 125.0513s\n",
            "#21 Loss: 0.0129 Acc: 99.7330% Time: 130.9002s\n",
            "#22 Loss: 0.0187 Acc: 99.5995% Time: 136.8891s\n",
            "#23 Loss: 0.0355 Acc: 98.6649% Time: 142.7764s\n",
            "#24 Loss: 0.0337 Acc: 98.6649% Time: 148.8274s\n",
            "#25 Loss: 0.0312 Acc: 99.0654% Time: 154.8780s\n",
            "#26 Loss: 0.0214 Acc: 99.3324% Time: 160.6351s\n",
            "#27 Loss: 0.0117 Acc: 99.8665% Time: 166.7697s\n",
            "#28 Loss: 0.0195 Acc: 98.9319% Time: 172.7628s\n",
            "#29 Loss: 0.0095 Acc: 99.8665% Time: 178.7888s\n",
            "#30 Loss: 0.0110 Acc: 99.7330% Time: 184.8051s\n",
            "#31 Loss: 0.0048 Acc: 99.8665% Time: 190.5568s\n",
            "#32 Loss: 0.0063 Acc: 100.0000% Time: 196.4520s\n",
            "#33 Loss: 0.0024 Acc: 100.0000% Time: 202.1852s\n",
            "#34 Loss: 0.0041 Acc: 100.0000% Time: 208.1445s\n",
            "#35 Loss: 0.0190 Acc: 99.4660% Time: 214.0789s\n",
            "#36 Loss: 0.0058 Acc: 99.8665% Time: 220.4015s\n",
            "#37 Loss: 0.0366 Acc: 98.9319% Time: 226.4043s\n",
            "#38 Loss: 0.0211 Acc: 99.1989% Time: 232.2101s\n",
            "#39 Loss: 0.0101 Acc: 99.7330% Time: 238.2642s\n",
            "#40 Loss: 0.0048 Acc: 100.0000% Time: 244.2059s\n",
            "#41 Loss: 0.0153 Acc: 99.8665% Time: 250.1099s\n",
            "#42 Loss: 0.0087 Acc: 99.7330% Time: 255.9435s\n",
            "#43 Loss: 0.0104 Acc: 99.5995% Time: 261.8276s\n",
            "#44 Loss: 0.0148 Acc: 99.4660% Time: 267.6939s\n",
            "#45 Loss: 0.0126 Acc: 99.5995% Time: 273.5574s\n",
            "#46 Loss: 0.0035 Acc: 100.0000% Time: 279.6033s\n",
            "#47 Loss: 0.0058 Acc: 99.8665% Time: 285.3811s\n",
            "#48 Loss: 0.0035 Acc: 100.0000% Time: 291.0105s\n",
            "#49 Loss: 0.0063 Acc: 99.8665% Time: 296.9015s\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.1858 Acc: 95.5645% Time: 1.0316s\n",
            "현재 Best는 epoch : 50 / iteration : 9번째 / Loss : 0.1858182937890712 \n"
          ]
        }
      ],
      "source": [
        "epochs = [50, 100, 150]\n",
        "for i in range(9, 10):\n",
        "    # set 9에서 j 번 째 학습을 의미\n",
        "    for j in range(1, 10):\n",
        "        # 데이터셋을 불러올 때 사용할 변형(transformation) 객체 정의\n",
        "        # 이미지의 밝기(brightness), 대비(contrast), 채도(saturation), 색조(hue)를 일부 변경\n",
        "        transforms_train = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation)\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)\n",
        "        ])\n",
        "\n",
        "        transforms_test = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        data_dir = '/home/tutor/안재형/dog_pic/닥스훈트'\n",
        "        data_folder = os.path.join(data_dir, f'set_{i}')\n",
        "        train_datasets = datasets.ImageFolder(os.path.join(data_folder, 'train'), transforms_train)\n",
        "        test_datasets = datasets.ImageFolder(os.path.join(data_folder, 'test'), transforms_test)\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=10, shuffle=True, num_workers=2)\n",
        "        test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=10, shuffle=True, num_workers=2)\n",
        "\n",
        "#         print('학습 데이터셋 크기:', len(train_datasets))\n",
        "#         print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "        class_names = train_datasets.classes\n",
        "#         print('클래스:', class_names)\n",
        "\n",
        "        #--------------------------------------------#\n",
        "\n",
        "        model = models.resnet34(pretrained=True)\n",
        "        num_features = model.fc.in_features\n",
        "        # 전이 학습(transfer learning): 모델의 출력 뉴런 수를 2개로 교체하여 마지막 레이어 다시 학습\n",
        "        model.fc = nn.Linear(num_features, 2)\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "        #--------------------------------------------#\n",
        "        print(f'-------------현재 set_{i} {j}번째 학습-------------')\n",
        "        num_epochs = epochs[j % 3]  # 1,2,0 , ...\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 전체 반복(epoch) 수 만큼 반복하며\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.\n",
        "            running_corrects = 0\n",
        "\n",
        "            # 배치 단위로 학습 데이터 불러오기\n",
        "            for inputs, labels in train_dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # 모델에 입력(forward)하고 결과 계산\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(train_datasets)\n",
        "            epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "\n",
        "            # 학습 과정 중에 결과 출력\n",
        "            print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "        #--------------------------------------------#\n",
        "\n",
        "        model.eval()\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0.\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in test_dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                # 한 배치의 첫 번째 이미지에 대하여 결과 시각화\n",
        "                print(f'[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})')\n",
        "    #             imshow(inputs.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
        "\n",
        "            epoch_loss = running_loss / len(test_datasets)\n",
        "            epoch_acc = running_corrects / len(test_datasets) * 100.\n",
        "            print('[Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "            Loss_Accuracy.append([i, j, epoch_loss, epoch_acc])\n",
        "        \n",
        "        # 8*10개 모든 모델을 저장하지 않고 가장 좋은 성능의 모델을 저장\n",
        "        if j == 1:\n",
        "            Best = [j, epoch_loss, epoch_acc]\n",
        "            # 닥스훈트 set i / j 번째 학습 1인 경우를 의미\n",
        "            torch.save(model, f'dac_set{i}_{j}_1.pth')\n",
        "\n",
        "        elif epoch_loss < Best[2]:\n",
        "            # Best 모델보다 Loss가 개선되면 저장\n",
        "            Best = [j, epoch_loss, epoch_acc]\n",
        "            # 가장 마지막으로 저장된 모델이 제일 좋은 성능\n",
        "            torch.save(model, f'dac_set{i}_{j}_1.pth')\n",
        "        print(f'현재 Best는 epoch : {epoch+1} / iteration : {Best[0]}번째 / Loss : {Best[1]} ')\n",
        "#         print('--------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8381989",
        "outputId": "1b91fe6c-05c3-475f-896f-14b2ff6fad2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "현재 Best는 epoch : 50 / iteration : 9번째 / Loss : 0.1858182937890712 \n"
          ]
        }
      ],
      "source": [
        "print(f'현재 Best는 epoch : {epoch+1} / iteration : {Best[0]}번째 / Loss : {Best[1]} ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "e222945e",
        "outputId": "a3c8dcc0-af47-417c-9b79-8eb978ba9323"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[9, 1, 0.1535915126721281, tensor(93.5484, device='cuda:0')],\n",
              " [9, 1, 0.17863983664767968, tensor(93.9516, device='cuda:0')],\n",
              " [9, 2, 0.2933852207833326, tensor(91.1290, device='cuda:0')],\n",
              " [9, 3, 0.20758715865574026, tensor(94.3548, device='cuda:0')],\n",
              " [9, 4, 0.20669178503287125, tensor(93.9516, device='cuda:0')],\n",
              " [9, 5, 0.28108396162937305, tensor(94.3548, device='cuda:0')],\n",
              " [9, 6, 0.24342889199359038, tensor(93.5484, device='cuda:0')],\n",
              " [9, 7, 0.18509611624458264, tensor(95.1613, device='cuda:0')],\n",
              " [9, 8, 0.20722513832820808, tensor(91.9355, device='cuda:0')],\n",
              " [9, 9, 0.1858182937890712, tensor(95.5645, device='cuda:0')]]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Loss_Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f06e2c1e"
      },
      "source": [
        "- 평균적으로 epoch = 100에서 가장 좋은 성능을 보인다.\n",
        "- 학습 2는 epoch = 100을 기준으로 batch_size를 8 ~ 12까지 확인해보자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43b7100b"
      },
      "source": [
        "# 학습 2\n",
        "- epoch = 100\n",
        "- batch_size = range(8, 13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b93d3909"
      },
      "outputs": [],
      "source": [
        "# Loss_Accuracy에 각 배치 사이즈마다의 성능을 넣어둔다\n",
        "Loss_Accuracy = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53859443",
        "outputId": "3ab8971d-6d5d-4968-eb81-acddb09fe91e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------현재 set_9 batch_size는 8-------------\n",
            "#0 Loss: 0.5231 Acc: 76.3685% Time: 6.3759s\n",
            "#1 Loss: 0.3159 Acc: 85.8478% Time: 12.3340s\n",
            "#2 Loss: 0.2646 Acc: 89.4526% Time: 18.2609s\n",
            "#3 Loss: 0.3057 Acc: 88.5180% Time: 24.3093s\n",
            "#4 Loss: 0.1687 Acc: 93.4579% Time: 30.2783s\n",
            "#5 Loss: 0.1248 Acc: 95.7276% Time: 36.3685s\n",
            "#6 Loss: 0.0990 Acc: 97.1963% Time: 42.4939s\n",
            "#7 Loss: 0.1031 Acc: 97.0628% Time: 48.4699s\n",
            "#8 Loss: 0.0816 Acc: 96.5287% Time: 54.4856s\n",
            "#9 Loss: 0.0885 Acc: 96.6622% Time: 60.6731s\n",
            "#10 Loss: 0.0864 Acc: 97.1963% Time: 66.4886s\n",
            "#11 Loss: 0.1304 Acc: 96.3952% Time: 72.3987s\n",
            "#12 Loss: 0.0717 Acc: 97.1963% Time: 78.2490s\n",
            "#13 Loss: 0.0477 Acc: 97.5968% Time: 84.4638s\n",
            "#14 Loss: 0.0277 Acc: 99.1989% Time: 90.5158s\n",
            "#15 Loss: 0.0501 Acc: 97.9973% Time: 96.7152s\n",
            "#16 Loss: 0.0451 Acc: 98.3979% Time: 102.7762s\n",
            "#17 Loss: 0.0359 Acc: 98.7984% Time: 108.7042s\n",
            "#18 Loss: 0.0336 Acc: 98.9319% Time: 114.5620s\n",
            "#19 Loss: 0.0171 Acc: 99.5995% Time: 120.3255s\n",
            "#20 Loss: 0.0239 Acc: 99.3324% Time: 126.3486s\n",
            "#21 Loss: 0.0208 Acc: 99.0654% Time: 132.3652s\n",
            "#22 Loss: 0.0154 Acc: 99.8665% Time: 138.4029s\n",
            "#23 Loss: 0.0240 Acc: 99.4660% Time: 144.2449s\n",
            "#24 Loss: 0.0097 Acc: 99.8665% Time: 150.4419s\n",
            "#25 Loss: 0.0348 Acc: 99.1989% Time: 156.5182s\n",
            "#26 Loss: 0.0430 Acc: 98.5314% Time: 162.4425s\n",
            "#27 Loss: 0.0238 Acc: 99.0654% Time: 168.6031s\n",
            "#28 Loss: 0.0153 Acc: 99.5995% Time: 174.5835s\n",
            "#29 Loss: 0.0248 Acc: 98.9319% Time: 180.5983s\n",
            "#30 Loss: 0.0204 Acc: 99.4660% Time: 186.5736s\n",
            "#31 Loss: 0.0076 Acc: 99.8665% Time: 192.6557s\n",
            "#32 Loss: 0.0078 Acc: 99.7330% Time: 198.6281s\n",
            "#33 Loss: 0.0081 Acc: 99.8665% Time: 204.6402s\n",
            "#34 Loss: 0.0177 Acc: 99.4660% Time: 210.9910s\n",
            "#35 Loss: 0.0264 Acc: 99.1989% Time: 216.9387s\n",
            "#36 Loss: 0.0367 Acc: 98.7984% Time: 223.0664s\n",
            "#37 Loss: 0.0185 Acc: 99.3324% Time: 229.0893s\n",
            "#38 Loss: 0.0291 Acc: 99.0654% Time: 235.1467s\n",
            "#39 Loss: 0.0282 Acc: 99.1989% Time: 241.4901s\n",
            "#40 Loss: 0.0289 Acc: 99.0654% Time: 247.7082s\n",
            "#41 Loss: 0.0169 Acc: 99.7330% Time: 253.7588s\n",
            "#42 Loss: 0.0100 Acc: 99.5995% Time: 259.8376s\n",
            "#43 Loss: 0.0060 Acc: 99.8665% Time: 265.9439s\n",
            "#44 Loss: 0.0075 Acc: 99.8665% Time: 272.0849s\n",
            "#45 Loss: 0.0193 Acc: 99.5995% Time: 278.4416s\n",
            "#46 Loss: 0.0045 Acc: 99.8665% Time: 284.4691s\n",
            "#47 Loss: 0.0047 Acc: 100.0000% Time: 290.6283s\n",
            "#48 Loss: 0.0023 Acc: 100.0000% Time: 296.7253s\n",
            "#49 Loss: 0.0177 Acc: 99.3324% Time: 302.8402s\n",
            "#50 Loss: 0.0044 Acc: 99.8665% Time: 308.8983s\n",
            "#51 Loss: 0.0086 Acc: 99.7330% Time: 315.0950s\n",
            "#52 Loss: 0.0032 Acc: 99.8665% Time: 321.2832s\n",
            "#53 Loss: 0.0022 Acc: 100.0000% Time: 327.3129s\n",
            "#54 Loss: 0.0024 Acc: 100.0000% Time: 333.6394s\n",
            "#55 Loss: 0.0026 Acc: 100.0000% Time: 340.1771s\n",
            "#56 Loss: 0.0029 Acc: 99.8665% Time: 346.1849s\n",
            "#57 Loss: 0.0067 Acc: 99.7330% Time: 352.4316s\n",
            "#58 Loss: 0.0950 Acc: 98.1308% Time: 358.5961s\n",
            "#59 Loss: 0.0273 Acc: 98.9319% Time: 364.6803s\n",
            "#60 Loss: 0.0078 Acc: 99.7330% Time: 370.8160s\n",
            "#61 Loss: 0.0041 Acc: 100.0000% Time: 376.9824s\n",
            "#62 Loss: 0.0058 Acc: 99.7330% Time: 383.3952s\n",
            "#63 Loss: 0.0053 Acc: 99.8665% Time: 389.5537s\n",
            "#64 Loss: 0.0021 Acc: 100.0000% Time: 395.6183s\n",
            "#65 Loss: 0.0014 Acc: 100.0000% Time: 401.7686s\n",
            "#66 Loss: 0.0027 Acc: 100.0000% Time: 407.9393s\n",
            "#67 Loss: 0.0007 Acc: 100.0000% Time: 414.3183s\n",
            "#68 Loss: 0.0008 Acc: 100.0000% Time: 420.4578s\n",
            "#69 Loss: 0.0011 Acc: 100.0000% Time: 426.3698s\n",
            "#70 Loss: 0.0018 Acc: 100.0000% Time: 432.7075s\n",
            "#71 Loss: 0.0009 Acc: 100.0000% Time: 438.9958s\n",
            "#72 Loss: 0.0007 Acc: 100.0000% Time: 445.2673s\n",
            "#73 Loss: 0.0024 Acc: 99.8665% Time: 451.6225s\n",
            "#74 Loss: 0.0024 Acc: 100.0000% Time: 457.8246s\n",
            "#75 Loss: 0.0007 Acc: 100.0000% Time: 464.0469s\n",
            "#76 Loss: 0.0013 Acc: 100.0000% Time: 470.2369s\n",
            "#77 Loss: 0.0010 Acc: 100.0000% Time: 476.3362s\n",
            "#78 Loss: 0.0063 Acc: 99.5995% Time: 482.3940s\n",
            "#79 Loss: 0.0016 Acc: 100.0000% Time: 488.5458s\n",
            "#80 Loss: 0.0006 Acc: 100.0000% Time: 494.6995s\n",
            "#81 Loss: 0.0018 Acc: 99.8665% Time: 501.0917s\n",
            "#82 Loss: 0.0006 Acc: 100.0000% Time: 507.2839s\n",
            "#83 Loss: 0.0190 Acc: 99.1989% Time: 513.6707s\n",
            "#84 Loss: 0.0038 Acc: 99.8665% Time: 519.8658s\n",
            "#85 Loss: 0.0022 Acc: 100.0000% Time: 526.2080s\n",
            "#86 Loss: 0.0011 Acc: 100.0000% Time: 532.2926s\n",
            "#87 Loss: 0.0015 Acc: 100.0000% Time: 538.3270s\n",
            "#88 Loss: 0.0019 Acc: 100.0000% Time: 544.4876s\n",
            "#89 Loss: 0.0007 Acc: 100.0000% Time: 551.0407s\n",
            "#90 Loss: 0.0035 Acc: 99.8665% Time: 557.2802s\n",
            "#91 Loss: 0.0008 Acc: 100.0000% Time: 563.4261s\n",
            "#92 Loss: 0.0007 Acc: 100.0000% Time: 569.6845s\n",
            "#93 Loss: 0.0003 Acc: 100.0000% Time: 575.8722s\n",
            "#94 Loss: 0.0005 Acc: 100.0000% Time: 582.0227s\n",
            "#95 Loss: 0.0007 Acc: 100.0000% Time: 588.1564s\n",
            "#96 Loss: 0.0005 Acc: 100.0000% Time: 594.5781s\n",
            "#97 Loss: 0.0004 Acc: 100.0000% Time: 600.8971s\n",
            "#98 Loss: 0.0003 Acc: 100.0000% Time: 607.0766s\n",
            "#99 Loss: 0.0013 Acc: 99.8665% Time: 613.2384s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2504 Acc: 92.7419% Time: 1.1724s\n",
            "현재 Best는 batch_szie : 8 / Loss : 0.2503840300314257 / Acc : 92.74192810058594\n",
            "-------------현재 set_9 batch_size는 9-------------\n",
            "#0 Loss: 0.4378 Acc: 77.8371% Time: 5.9707s\n",
            "#1 Loss: 0.3034 Acc: 87.4499% Time: 12.0074s\n",
            "#2 Loss: 0.2180 Acc: 90.9212% Time: 18.0160s\n",
            "#3 Loss: 0.1687 Acc: 93.0574% Time: 24.1170s\n",
            "#4 Loss: 0.1169 Acc: 95.8612% Time: 30.1382s\n",
            "#5 Loss: 0.0952 Acc: 96.7957% Time: 36.2968s\n",
            "#6 Loss: 0.1194 Acc: 95.3271% Time: 42.2157s\n",
            "#7 Loss: 0.1151 Acc: 95.5941% Time: 48.0837s\n",
            "#8 Loss: 0.1321 Acc: 95.1936% Time: 54.2356s\n",
            "#9 Loss: 0.0582 Acc: 97.5968% Time: 60.3902s\n",
            "#10 Loss: 0.1038 Acc: 96.3952% Time: 66.4460s\n",
            "#11 Loss: 0.0867 Acc: 96.6622% Time: 72.7059s\n",
            "#12 Loss: 0.0584 Acc: 98.3979% Time: 78.6805s\n",
            "#13 Loss: 0.0412 Acc: 98.1308% Time: 84.7398s\n",
            "#14 Loss: 0.0686 Acc: 97.7303% Time: 90.6349s\n",
            "#15 Loss: 0.0298 Acc: 98.9319% Time: 96.8339s\n",
            "#16 Loss: 0.0304 Acc: 98.9319% Time: 103.0279s\n",
            "#17 Loss: 0.0321 Acc: 98.9319% Time: 109.2521s\n",
            "#18 Loss: 0.0254 Acc: 99.3324% Time: 115.5243s\n",
            "#19 Loss: 0.0171 Acc: 99.4660% Time: 121.6179s\n",
            "#20 Loss: 0.0416 Acc: 98.6649% Time: 127.8074s\n",
            "#21 Loss: 0.0307 Acc: 98.9319% Time: 133.6655s\n",
            "#22 Loss: 0.0275 Acc: 98.9319% Time: 139.6317s\n",
            "#23 Loss: 0.0277 Acc: 99.3324% Time: 145.4848s\n",
            "#24 Loss: 0.0238 Acc: 99.3324% Time: 151.5698s\n",
            "#25 Loss: 0.0075 Acc: 100.0000% Time: 157.7408s\n",
            "#26 Loss: 0.0285 Acc: 98.9319% Time: 163.8738s\n",
            "#27 Loss: 0.0237 Acc: 99.3324% Time: 169.8929s\n",
            "#28 Loss: 0.0081 Acc: 99.7330% Time: 175.8051s\n",
            "#29 Loss: 0.0101 Acc: 99.7330% Time: 181.8364s\n",
            "#30 Loss: 0.1165 Acc: 97.9973% Time: 188.0047s\n",
            "#31 Loss: 0.0522 Acc: 98.7984% Time: 193.9068s\n",
            "#32 Loss: 0.0379 Acc: 98.5314% Time: 200.0971s\n",
            "#33 Loss: 0.0198 Acc: 99.5995% Time: 205.9486s\n",
            "#34 Loss: 0.0207 Acc: 99.5995% Time: 211.8920s\n",
            "#35 Loss: 0.0143 Acc: 99.5995% Time: 217.8606s\n",
            "#36 Loss: 0.0105 Acc: 99.7330% Time: 223.9832s\n",
            "#37 Loss: 0.0071 Acc: 99.7330% Time: 230.1616s\n",
            "#38 Loss: 0.0108 Acc: 99.5995% Time: 236.3584s\n",
            "#39 Loss: 0.0069 Acc: 99.7330% Time: 242.4096s\n",
            "#40 Loss: 0.0415 Acc: 99.0654% Time: 248.8192s\n",
            "#41 Loss: 0.0257 Acc: 99.1989% Time: 255.0473s\n",
            "#42 Loss: 0.0071 Acc: 99.7330% Time: 261.0144s\n",
            "#43 Loss: 0.0034 Acc: 100.0000% Time: 266.9702s\n",
            "#44 Loss: 0.0025 Acc: 100.0000% Time: 272.8209s\n",
            "#45 Loss: 0.0111 Acc: 99.3324% Time: 278.8924s\n",
            "#46 Loss: 0.0049 Acc: 99.8665% Time: 284.7811s\n",
            "#47 Loss: 0.0062 Acc: 99.8665% Time: 290.6304s\n",
            "#48 Loss: 0.0036 Acc: 99.8665% Time: 296.6589s\n",
            "#49 Loss: 0.0100 Acc: 99.4660% Time: 302.8828s\n",
            "#50 Loss: 0.0086 Acc: 99.7330% Time: 309.1682s\n",
            "#51 Loss: 0.0080 Acc: 99.8665% Time: 315.4770s\n",
            "#52 Loss: 0.0060 Acc: 99.7330% Time: 321.6951s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#53 Loss: 0.0096 Acc: 99.8665% Time: 327.7936s\n",
            "#54 Loss: 0.0063 Acc: 99.8665% Time: 333.9800s\n",
            "#55 Loss: 0.0250 Acc: 99.7330% Time: 340.2289s\n",
            "#56 Loss: 0.0200 Acc: 99.5995% Time: 346.2831s\n",
            "#57 Loss: 0.0038 Acc: 99.8665% Time: 352.2505s\n",
            "#58 Loss: 0.0042 Acc: 99.8665% Time: 358.0617s\n",
            "#59 Loss: 0.0023 Acc: 100.0000% Time: 364.2545s\n",
            "#60 Loss: 0.0027 Acc: 99.8665% Time: 370.0769s\n",
            "#61 Loss: 0.0024 Acc: 100.0000% Time: 375.7938s\n",
            "#62 Loss: 0.0022 Acc: 100.0000% Time: 381.9275s\n",
            "#63 Loss: 0.0011 Acc: 100.0000% Time: 387.8766s\n",
            "#64 Loss: 0.0010 Acc: 100.0000% Time: 393.8655s\n",
            "#65 Loss: 0.0007 Acc: 100.0000% Time: 399.9237s\n",
            "#66 Loss: 0.0033 Acc: 99.8665% Time: 406.0604s\n",
            "#67 Loss: 0.0111 Acc: 99.8665% Time: 412.2196s\n",
            "#68 Loss: 0.0023 Acc: 100.0000% Time: 418.2316s\n",
            "#69 Loss: 0.0021 Acc: 100.0000% Time: 424.2495s\n",
            "#70 Loss: 0.0265 Acc: 99.5995% Time: 430.4599s\n",
            "#71 Loss: 0.0157 Acc: 99.1989% Time: 436.4819s\n",
            "#72 Loss: 0.0030 Acc: 99.8665% Time: 442.4706s\n",
            "#73 Loss: 0.0020 Acc: 100.0000% Time: 448.6712s\n",
            "#74 Loss: 0.0023 Acc: 100.0000% Time: 454.7815s\n",
            "#75 Loss: 0.0013 Acc: 100.0000% Time: 460.8599s\n",
            "#76 Loss: 0.0014 Acc: 100.0000% Time: 466.7061s\n",
            "#77 Loss: 0.0009 Acc: 100.0000% Time: 472.9649s\n",
            "#78 Loss: 0.0009 Acc: 100.0000% Time: 479.4534s\n",
            "#79 Loss: 0.0135 Acc: 99.7330% Time: 485.4315s\n",
            "#80 Loss: 0.0037 Acc: 100.0000% Time: 491.8059s\n",
            "#81 Loss: 0.0014 Acc: 100.0000% Time: 497.9227s\n",
            "#82 Loss: 0.0015 Acc: 100.0000% Time: 503.8921s\n",
            "#83 Loss: 0.0026 Acc: 99.8665% Time: 509.7591s\n",
            "#84 Loss: 0.0028 Acc: 99.8665% Time: 515.6341s\n",
            "#85 Loss: 0.0200 Acc: 99.7330% Time: 521.8412s\n",
            "#86 Loss: 0.0223 Acc: 98.9319% Time: 527.8054s\n",
            "#87 Loss: 0.0039 Acc: 99.8665% Time: 533.8033s\n",
            "#88 Loss: 0.0014 Acc: 100.0000% Time: 539.6155s\n",
            "#89 Loss: 0.0098 Acc: 99.5995% Time: 545.7147s\n",
            "#90 Loss: 0.0079 Acc: 99.7330% Time: 551.7930s\n",
            "#91 Loss: 0.0201 Acc: 99.7330% Time: 557.7804s\n",
            "#92 Loss: 0.0222 Acc: 99.4660% Time: 563.7523s\n",
            "#93 Loss: 0.0037 Acc: 100.0000% Time: 569.8006s\n",
            "#94 Loss: 0.0033 Acc: 99.8665% Time: 576.0197s\n",
            "#95 Loss: 0.0022 Acc: 100.0000% Time: 582.1342s\n",
            "#96 Loss: 0.0011 Acc: 100.0000% Time: 588.2059s\n",
            "#97 Loss: 0.0017 Acc: 100.0000% Time: 594.3425s\n",
            "#98 Loss: 0.0009 Acc: 100.0000% Time: 600.5796s\n",
            "#99 Loss: 0.0009 Acc: 100.0000% Time: 606.8354s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.1545 Acc: 94.3548% Time: 1.1196s\n",
            "현재 Best는 batch_szie : 9 / Loss : 0.1545006600478055 / Acc : 94.3548355102539\n",
            "-------------현재 set_9 batch_size는 10-------------\n",
            "#0 Loss: 0.5038 Acc: 75.4339% Time: 6.0559s\n",
            "#1 Loss: 0.3116 Acc: 87.5835% Time: 11.9888s\n",
            "#2 Loss: 0.2021 Acc: 91.9893% Time: 17.7988s\n",
            "#3 Loss: 0.2149 Acc: 91.9893% Time: 23.9781s\n",
            "#4 Loss: 0.1809 Acc: 92.9239% Time: 29.9314s\n",
            "#5 Loss: 0.1226 Acc: 95.4606% Time: 35.8207s\n",
            "#6 Loss: 0.1022 Acc: 96.5287% Time: 41.5169s\n",
            "#7 Loss: 0.0684 Acc: 97.8638% Time: 47.3707s\n",
            "#8 Loss: 0.1136 Acc: 95.7276% Time: 53.2214s\n",
            "#9 Loss: 0.0603 Acc: 97.9973% Time: 59.1078s\n",
            "#10 Loss: 0.0351 Acc: 99.1989% Time: 64.8128s\n",
            "#11 Loss: 0.0464 Acc: 97.9973% Time: 70.6101s\n",
            "#12 Loss: 0.0490 Acc: 98.5314% Time: 76.5831s\n",
            "#13 Loss: 0.0490 Acc: 98.5314% Time: 82.5583s\n",
            "#14 Loss: 0.0230 Acc: 99.4660% Time: 88.4459s\n",
            "#15 Loss: 0.0343 Acc: 98.9319% Time: 94.5474s\n",
            "#16 Loss: 0.0255 Acc: 99.3324% Time: 100.4165s\n",
            "#17 Loss: 0.0262 Acc: 99.0654% Time: 106.3137s\n",
            "#18 Loss: 0.0439 Acc: 98.5314% Time: 112.2827s\n",
            "#19 Loss: 0.0243 Acc: 99.4660% Time: 118.1396s\n",
            "#20 Loss: 0.0479 Acc: 98.7984% Time: 123.7832s\n",
            "#21 Loss: 0.0352 Acc: 98.9319% Time: 129.5205s\n",
            "#22 Loss: 0.0120 Acc: 99.5995% Time: 135.3167s\n",
            "#23 Loss: 0.0163 Acc: 99.4660% Time: 141.3346s\n",
            "#24 Loss: 0.0261 Acc: 99.1989% Time: 146.9683s\n",
            "#25 Loss: 0.0048 Acc: 100.0000% Time: 152.8715s\n",
            "#26 Loss: 0.0036 Acc: 100.0000% Time: 158.6950s\n",
            "#27 Loss: 0.0031 Acc: 100.0000% Time: 164.3959s\n",
            "#28 Loss: 0.0415 Acc: 98.6649% Time: 169.9340s\n",
            "#29 Loss: 0.0275 Acc: 99.3324% Time: 175.8140s\n",
            "#30 Loss: 0.0104 Acc: 99.5995% Time: 181.5106s\n",
            "#31 Loss: 0.0060 Acc: 99.8665% Time: 187.4895s\n",
            "#32 Loss: 0.0170 Acc: 99.4660% Time: 193.5176s\n",
            "#33 Loss: 0.0057 Acc: 99.8665% Time: 199.2774s\n",
            "#34 Loss: 0.0115 Acc: 99.5995% Time: 205.2368s\n",
            "#35 Loss: 0.0195 Acc: 99.5995% Time: 211.1123s\n",
            "#36 Loss: 0.0189 Acc: 99.1989% Time: 216.8486s\n",
            "#37 Loss: 0.0071 Acc: 99.8665% Time: 222.6245s\n",
            "#38 Loss: 0.0099 Acc: 99.5995% Time: 228.3716s\n",
            "#39 Loss: 0.0034 Acc: 100.0000% Time: 234.2762s\n",
            "#40 Loss: 0.0170 Acc: 99.7330% Time: 240.0504s\n",
            "#41 Loss: 0.0180 Acc: 99.4660% Time: 245.8565s\n",
            "#42 Loss: 0.0035 Acc: 100.0000% Time: 251.4024s\n",
            "#43 Loss: 0.0039 Acc: 99.8665% Time: 257.4224s\n",
            "#44 Loss: 0.0156 Acc: 99.4660% Time: 263.2591s\n",
            "#45 Loss: 0.0052 Acc: 99.8665% Time: 269.0303s\n",
            "#46 Loss: 0.0094 Acc: 99.7330% Time: 274.7849s\n",
            "#47 Loss: 0.0135 Acc: 99.5995% Time: 280.6205s\n",
            "#48 Loss: 0.0029 Acc: 100.0000% Time: 286.2295s\n",
            "#49 Loss: 0.0028 Acc: 100.0000% Time: 291.7136s\n",
            "#50 Loss: 0.0114 Acc: 99.4660% Time: 297.4484s\n",
            "#51 Loss: 0.0057 Acc: 99.8665% Time: 303.2353s\n",
            "#52 Loss: 0.0044 Acc: 99.8665% Time: 308.8035s\n",
            "#53 Loss: 0.0104 Acc: 99.8665% Time: 314.3075s\n",
            "#54 Loss: 0.0055 Acc: 99.8665% Time: 320.1148s\n",
            "#55 Loss: 0.0081 Acc: 99.8665% Time: 325.8391s\n",
            "#56 Loss: 0.0041 Acc: 100.0000% Time: 331.6648s\n",
            "#57 Loss: 0.0020 Acc: 100.0000% Time: 337.2996s\n",
            "#58 Loss: 0.0117 Acc: 99.5995% Time: 343.2870s\n",
            "#59 Loss: 0.0023 Acc: 100.0000% Time: 349.1219s\n",
            "#60 Loss: 0.0023 Acc: 100.0000% Time: 355.1848s\n",
            "#61 Loss: 0.0021 Acc: 100.0000% Time: 361.0257s\n",
            "#62 Loss: 0.0013 Acc: 100.0000% Time: 366.8030s\n",
            "#63 Loss: 0.0010 Acc: 100.0000% Time: 372.5993s\n",
            "#64 Loss: 0.0010 Acc: 100.0000% Time: 378.2543s\n",
            "#65 Loss: 0.0048 Acc: 99.8665% Time: 383.6930s\n",
            "#66 Loss: 0.0015 Acc: 100.0000% Time: 389.7402s\n",
            "#67 Loss: 0.0012 Acc: 100.0000% Time: 395.2378s\n",
            "#68 Loss: 0.0009 Acc: 100.0000% Time: 400.9361s\n",
            "#69 Loss: 0.0017 Acc: 99.8665% Time: 406.7758s\n",
            "#70 Loss: 0.0010 Acc: 100.0000% Time: 412.5762s\n",
            "#71 Loss: 0.0011 Acc: 100.0000% Time: 418.4910s\n",
            "#72 Loss: 0.0011 Acc: 100.0000% Time: 424.2441s\n",
            "#73 Loss: 0.0008 Acc: 100.0000% Time: 430.2435s\n",
            "#74 Loss: 0.0005 Acc: 100.0000% Time: 436.0409s\n",
            "#75 Loss: 0.0013 Acc: 100.0000% Time: 441.8351s\n",
            "#76 Loss: 0.0005 Acc: 100.0000% Time: 447.5047s\n",
            "#77 Loss: 0.0006 Acc: 100.0000% Time: 453.1080s\n",
            "#78 Loss: 0.0006 Acc: 100.0000% Time: 458.7518s\n",
            "#79 Loss: 0.0015 Acc: 100.0000% Time: 464.7850s\n",
            "#80 Loss: 0.0010 Acc: 100.0000% Time: 470.2726s\n",
            "#81 Loss: 0.0012 Acc: 100.0000% Time: 476.0646s\n",
            "#82 Loss: 0.0039 Acc: 99.8665% Time: 481.8130s\n",
            "#83 Loss: 0.0016 Acc: 100.0000% Time: 487.6386s\n",
            "#84 Loss: 0.0041 Acc: 99.8665% Time: 493.5429s\n",
            "#85 Loss: 0.0066 Acc: 99.7330% Time: 499.1333s\n",
            "#86 Loss: 0.0038 Acc: 99.8665% Time: 504.6914s\n",
            "#87 Loss: 0.0045 Acc: 99.8665% Time: 510.1951s\n",
            "#88 Loss: 0.0026 Acc: 100.0000% Time: 515.9239s\n",
            "#89 Loss: 0.0072 Acc: 99.7330% Time: 521.9428s\n",
            "#90 Loss: 0.0022 Acc: 100.0000% Time: 527.8086s\n",
            "#91 Loss: 0.0008 Acc: 100.0000% Time: 533.5509s\n",
            "#92 Loss: 0.0006 Acc: 100.0000% Time: 539.2606s\n",
            "#93 Loss: 0.0016 Acc: 100.0000% Time: 545.2573s\n",
            "#94 Loss: 0.0004 Acc: 100.0000% Time: 551.0051s\n",
            "#95 Loss: 0.0007 Acc: 100.0000% Time: 556.9353s\n",
            "#96 Loss: 0.0026 Acc: 99.8665% Time: 562.5712s\n",
            "#97 Loss: 0.0034 Acc: 99.8665% Time: 568.2233s\n",
            "#98 Loss: 0.0017 Acc: 100.0000% Time: 573.9270s\n",
            "#99 Loss: 0.0047 Acc: 99.8665% Time: 579.5509s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[Test Phase] Loss: 0.2523 Acc: 92.3387% Time: 1.1333s\n",
            "현재 Best는 batch_szie : 9 / Loss : 0.1545006600478055 / Acc : 94.3548355102539\n",
            "-------------현재 set_9 batch_size는 11-------------\n",
            "#0 Loss: 0.4547 Acc: 79.9733% Time: 5.7947s\n",
            "#1 Loss: 0.3139 Acc: 85.8478% Time: 11.7417s\n",
            "#2 Loss: 0.2425 Acc: 90.3872% Time: 17.5350s\n",
            "#3 Loss: 0.1707 Acc: 94.1255% Time: 23.4073s\n",
            "#4 Loss: 0.1349 Acc: 94.9266% Time: 29.3323s\n",
            "#5 Loss: 0.1496 Acc: 94.2590% Time: 35.3591s\n",
            "#6 Loss: 0.1204 Acc: 95.7276% Time: 41.2723s\n",
            "#7 Loss: 0.0843 Acc: 97.0628% Time: 46.8406s\n",
            "#8 Loss: 0.0940 Acc: 96.5287% Time: 52.5882s\n",
            "#9 Loss: 0.1527 Acc: 94.3925% Time: 58.3613s\n",
            "#10 Loss: 0.0876 Acc: 95.7276% Time: 64.0832s\n",
            "#11 Loss: 0.0349 Acc: 98.6649% Time: 69.5951s\n",
            "#12 Loss: 0.0525 Acc: 98.1308% Time: 75.2760s\n",
            "#13 Loss: 0.0422 Acc: 98.7984% Time: 80.8807s\n",
            "#14 Loss: 0.0552 Acc: 98.2644% Time: 86.4920s\n",
            "#15 Loss: 0.0383 Acc: 99.0654% Time: 91.9095s\n",
            "#16 Loss: 0.0434 Acc: 98.6649% Time: 97.6474s\n",
            "#17 Loss: 0.0345 Acc: 98.9319% Time: 103.5296s\n",
            "#18 Loss: 0.0653 Acc: 97.7303% Time: 109.2586s\n",
            "#19 Loss: 0.0389 Acc: 98.3979% Time: 115.4269s\n",
            "#20 Loss: 0.0202 Acc: 99.3324% Time: 121.0137s\n",
            "#21 Loss: 0.0495 Acc: 97.8638% Time: 126.8110s\n",
            "#22 Loss: 0.0152 Acc: 99.4660% Time: 132.6275s\n",
            "#23 Loss: 0.0195 Acc: 99.1989% Time: 138.3928s\n",
            "#24 Loss: 0.0187 Acc: 99.3324% Time: 144.1808s\n",
            "#25 Loss: 0.0243 Acc: 98.9319% Time: 149.9357s\n",
            "#26 Loss: 0.0295 Acc: 98.7984% Time: 155.9116s\n",
            "#27 Loss: 0.0231 Acc: 99.1989% Time: 161.9856s\n",
            "#28 Loss: 0.0416 Acc: 98.6649% Time: 167.7488s\n",
            "#29 Loss: 0.0259 Acc: 99.0654% Time: 173.3687s\n",
            "#30 Loss: 0.0217 Acc: 99.1989% Time: 179.0492s\n",
            "#31 Loss: 0.0358 Acc: 98.7984% Time: 184.7800s\n",
            "#32 Loss: 0.0203 Acc: 99.0654% Time: 190.4559s\n",
            "#33 Loss: 0.0227 Acc: 99.0654% Time: 196.5895s\n",
            "#34 Loss: 0.0555 Acc: 97.7303% Time: 202.4354s\n",
            "#35 Loss: 0.0523 Acc: 97.8638% Time: 208.2409s\n",
            "#36 Loss: 0.0544 Acc: 98.2644% Time: 213.9697s\n",
            "#37 Loss: 0.0254 Acc: 99.0654% Time: 219.7782s\n",
            "#38 Loss: 0.0098 Acc: 99.7330% Time: 225.7986s\n",
            "#39 Loss: 0.0175 Acc: 99.0654% Time: 231.5487s\n",
            "#40 Loss: 0.0171 Acc: 99.1989% Time: 237.5795s\n",
            "#41 Loss: 0.0091 Acc: 99.7330% Time: 243.2596s\n",
            "#42 Loss: 0.0078 Acc: 99.8665% Time: 248.9498s\n",
            "#43 Loss: 0.0090 Acc: 99.8665% Time: 254.6463s\n",
            "#44 Loss: 0.0127 Acc: 99.5995% Time: 260.2716s\n",
            "#45 Loss: 0.0061 Acc: 99.8665% Time: 266.0332s\n",
            "#46 Loss: 0.0147 Acc: 99.5995% Time: 271.9851s\n",
            "#47 Loss: 0.0199 Acc: 99.3324% Time: 277.9458s\n",
            "#48 Loss: 0.0035 Acc: 100.0000% Time: 283.7789s\n",
            "#49 Loss: 0.0030 Acc: 100.0000% Time: 289.5441s\n",
            "#50 Loss: 0.0034 Acc: 100.0000% Time: 295.2727s\n",
            "#51 Loss: 0.0081 Acc: 99.5995% Time: 301.0708s\n",
            "#52 Loss: 0.0053 Acc: 99.8665% Time: 306.8179s\n",
            "#53 Loss: 0.0042 Acc: 99.8665% Time: 312.8172s\n",
            "#54 Loss: 0.0059 Acc: 99.7330% Time: 318.8324s\n",
            "#55 Loss: 0.0045 Acc: 100.0000% Time: 324.5807s\n",
            "#56 Loss: 0.0020 Acc: 100.0000% Time: 330.4819s\n",
            "#57 Loss: 0.0018 Acc: 100.0000% Time: 336.1672s\n",
            "#58 Loss: 0.0015 Acc: 100.0000% Time: 341.7501s\n",
            "#59 Loss: 0.0146 Acc: 99.7330% Time: 347.3571s\n",
            "#60 Loss: 0.0349 Acc: 99.0654% Time: 353.0909s\n",
            "#61 Loss: 0.0289 Acc: 98.9319% Time: 358.7582s\n",
            "#62 Loss: 0.0048 Acc: 99.7330% Time: 364.4577s\n",
            "#63 Loss: 0.0088 Acc: 99.7330% Time: 370.0627s\n",
            "#64 Loss: 0.0083 Acc: 99.5995% Time: 375.7451s\n",
            "#65 Loss: 0.0017 Acc: 100.0000% Time: 381.2104s\n",
            "#66 Loss: 0.0099 Acc: 99.8665% Time: 387.3137s\n",
            "#67 Loss: 0.0465 Acc: 98.2644% Time: 393.1531s\n",
            "#68 Loss: 0.0044 Acc: 99.8665% Time: 399.0188s\n",
            "#69 Loss: 0.0116 Acc: 99.8665% Time: 405.0057s\n",
            "#70 Loss: 0.0345 Acc: 98.6649% Time: 410.7565s\n",
            "#71 Loss: 0.0363 Acc: 98.7984% Time: 416.7842s\n",
            "#72 Loss: 0.0072 Acc: 99.8665% Time: 422.5866s\n",
            "#73 Loss: 0.0155 Acc: 99.4660% Time: 428.3870s\n",
            "#74 Loss: 0.0304 Acc: 98.9319% Time: 434.3254s\n",
            "#75 Loss: 0.0435 Acc: 98.7984% Time: 440.1685s\n",
            "#76 Loss: 0.0174 Acc: 99.3324% Time: 446.0031s\n",
            "#77 Loss: 0.0143 Acc: 99.7330% Time: 451.8524s\n",
            "#78 Loss: 0.0069 Acc: 99.7330% Time: 457.6411s\n",
            "#79 Loss: 0.0055 Acc: 100.0000% Time: 463.5473s\n",
            "#80 Loss: 0.0198 Acc: 99.1989% Time: 469.4260s\n",
            "#81 Loss: 0.0210 Acc: 99.0654% Time: 475.3597s\n",
            "#82 Loss: 0.0164 Acc: 99.1989% Time: 481.0525s\n",
            "#83 Loss: 0.0191 Acc: 99.5995% Time: 486.7514s\n",
            "#84 Loss: 0.0044 Acc: 99.8665% Time: 492.7193s\n",
            "#85 Loss: 0.0105 Acc: 99.7330% Time: 498.6264s\n",
            "#86 Loss: 0.0235 Acc: 99.1989% Time: 504.6488s\n",
            "#87 Loss: 0.0355 Acc: 98.5314% Time: 510.4615s\n",
            "#88 Loss: 0.0138 Acc: 99.5995% Time: 516.1140s\n",
            "#89 Loss: 0.0087 Acc: 99.7330% Time: 521.7126s\n",
            "#90 Loss: 0.0329 Acc: 98.9319% Time: 527.5413s\n",
            "#91 Loss: 0.0559 Acc: 97.9973% Time: 533.5058s\n",
            "#92 Loss: 0.0195 Acc: 99.1989% Time: 539.1822s\n",
            "#93 Loss: 0.0280 Acc: 98.7984% Time: 545.1279s\n",
            "#94 Loss: 0.1039 Acc: 96.3952% Time: 550.9948s\n",
            "#95 Loss: 0.0084 Acc: 99.5995% Time: 556.7496s\n",
            "#96 Loss: 0.0032 Acc: 99.8665% Time: 562.3861s\n",
            "#97 Loss: 0.0051 Acc: 99.8665% Time: 568.3895s\n",
            "#98 Loss: 0.0049 Acc: 99.8665% Time: 574.2138s\n",
            "#99 Loss: 0.0031 Acc: 99.8665% Time: 579.9655s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2597 Acc: 93.1452% Time: 1.0270s\n",
            "현재 Best는 batch_szie : 9 / Loss : 0.1545006600478055 / Acc : 94.3548355102539\n",
            "-------------현재 set_9 batch_size는 12-------------\n",
            "#0 Loss: 0.4725 Acc: 77.8371% Time: 5.7458s\n",
            "#1 Loss: 0.2932 Acc: 88.2510% Time: 11.2483s\n",
            "#2 Loss: 0.2106 Acc: 91.1883% Time: 16.8855s\n",
            "#3 Loss: 0.1976 Acc: 92.6569% Time: 22.6381s\n",
            "#4 Loss: 0.1696 Acc: 93.8585% Time: 28.7604s\n",
            "#5 Loss: 0.1775 Acc: 92.7904% Time: 34.2593s\n",
            "#6 Loss: 0.0456 Acc: 98.6649% Time: 39.7847s\n",
            "#7 Loss: 0.0882 Acc: 96.7957% Time: 45.5239s\n",
            "#8 Loss: 0.0908 Acc: 96.2617% Time: 50.9991s\n",
            "#9 Loss: 0.0729 Acc: 97.1963% Time: 56.5400s\n",
            "#10 Loss: 0.0435 Acc: 98.6649% Time: 62.3597s\n",
            "#11 Loss: 0.0760 Acc: 96.7957% Time: 68.1427s\n",
            "#12 Loss: 0.0419 Acc: 98.7984% Time: 73.8457s\n",
            "#13 Loss: 0.0350 Acc: 99.1989% Time: 79.5173s\n",
            "#14 Loss: 0.0796 Acc: 97.8638% Time: 85.2261s\n",
            "#15 Loss: 0.0429 Acc: 97.9973% Time: 90.7050s\n",
            "#16 Loss: 0.0158 Acc: 99.5995% Time: 96.5655s\n",
            "#17 Loss: 0.0159 Acc: 99.7330% Time: 102.1814s\n",
            "#18 Loss: 0.0238 Acc: 99.1989% Time: 107.9124s\n",
            "#19 Loss: 0.0182 Acc: 99.5995% Time: 113.5932s\n",
            "#20 Loss: 0.0177 Acc: 99.5995% Time: 119.2359s\n",
            "#21 Loss: 0.0219 Acc: 99.4660% Time: 124.8955s\n",
            "#22 Loss: 0.0163 Acc: 99.4660% Time: 130.5053s\n",
            "#23 Loss: 0.0436 Acc: 98.6649% Time: 136.3140s\n",
            "#24 Loss: 0.0360 Acc: 99.1989% Time: 142.0940s\n",
            "#25 Loss: 0.0259 Acc: 99.3324% Time: 147.7200s\n",
            "#26 Loss: 0.0137 Acc: 99.7330% Time: 153.2701s\n",
            "#27 Loss: 0.0129 Acc: 99.7330% Time: 158.8603s\n",
            "#28 Loss: 0.0235 Acc: 99.3324% Time: 164.8622s\n",
            "#29 Loss: 0.0068 Acc: 99.8665% Time: 170.4767s\n",
            "#30 Loss: 0.0140 Acc: 99.5995% Time: 176.3854s\n",
            "#31 Loss: 0.0079 Acc: 99.8665% Time: 182.1911s\n",
            "#32 Loss: 0.0140 Acc: 99.4660% Time: 187.9085s\n",
            "#33 Loss: 0.0232 Acc: 98.9319% Time: 193.8394s\n",
            "#34 Loss: 0.0124 Acc: 99.7330% Time: 199.4616s\n",
            "#35 Loss: 0.0050 Acc: 99.8665% Time: 205.2045s\n",
            "#36 Loss: 0.0134 Acc: 99.4660% Time: 211.0753s\n",
            "#37 Loss: 0.0039 Acc: 99.8665% Time: 216.7348s\n",
            "#38 Loss: 0.0059 Acc: 99.8665% Time: 222.3415s\n",
            "#39 Loss: 0.0093 Acc: 99.8665% Time: 227.9944s\n",
            "#40 Loss: 0.0122 Acc: 99.4660% Time: 234.2988s\n",
            "#41 Loss: 0.0203 Acc: 99.1989% Time: 239.9547s\n",
            "#42 Loss: 0.0065 Acc: 99.8665% Time: 245.3962s\n",
            "#43 Loss: 0.0228 Acc: 99.4660% Time: 250.8308s\n",
            "#44 Loss: 0.0262 Acc: 98.9319% Time: 256.4963s\n",
            "#45 Loss: 0.0374 Acc: 98.9319% Time: 262.2056s\n",
            "#46 Loss: 0.0101 Acc: 99.7330% Time: 268.1026s\n",
            "#47 Loss: 0.0169 Acc: 99.3324% Time: 273.7213s\n",
            "#48 Loss: 0.0198 Acc: 99.3324% Time: 279.3895s\n",
            "#49 Loss: 0.0127 Acc: 99.5995% Time: 284.9389s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#50 Loss: 0.0106 Acc: 99.7330% Time: 290.9176s\n",
            "#51 Loss: 0.0065 Acc: 99.7330% Time: 296.5084s\n",
            "#52 Loss: 0.0033 Acc: 100.0000% Time: 302.1808s\n",
            "#53 Loss: 0.0041 Acc: 99.8665% Time: 307.7756s\n",
            "#54 Loss: 0.0282 Acc: 99.4660% Time: 313.7295s\n",
            "#55 Loss: 0.0104 Acc: 99.4660% Time: 319.3716s\n",
            "#56 Loss: 0.0135 Acc: 99.5995% Time: 325.0933s\n",
            "#57 Loss: 0.0065 Acc: 99.8665% Time: 331.0007s\n",
            "#58 Loss: 0.0109 Acc: 99.7330% Time: 336.7056s\n",
            "#59 Loss: 0.0230 Acc: 99.3324% Time: 342.3273s\n",
            "#60 Loss: 0.0103 Acc: 99.5995% Time: 348.3972s\n",
            "#61 Loss: 0.0043 Acc: 99.8665% Time: 353.8747s\n",
            "#62 Loss: 0.0052 Acc: 99.8665% Time: 359.5373s\n",
            "#63 Loss: 0.0057 Acc: 99.7330% Time: 365.3521s\n",
            "#64 Loss: 0.0043 Acc: 99.8665% Time: 370.7962s\n",
            "#65 Loss: 0.0120 Acc: 99.7330% Time: 376.4763s\n",
            "#66 Loss: 0.0030 Acc: 100.0000% Time: 382.2632s\n",
            "#67 Loss: 0.0031 Acc: 99.8665% Time: 387.7932s\n",
            "#68 Loss: 0.0023 Acc: 100.0000% Time: 393.6482s\n",
            "#69 Loss: 0.0018 Acc: 100.0000% Time: 399.3520s\n",
            "#70 Loss: 0.0016 Acc: 100.0000% Time: 405.0680s\n",
            "#71 Loss: 0.0014 Acc: 100.0000% Time: 410.6954s\n",
            "#72 Loss: 0.0009 Acc: 100.0000% Time: 416.1783s\n",
            "#73 Loss: 0.0019 Acc: 99.8665% Time: 422.1204s\n",
            "#74 Loss: 0.0031 Acc: 100.0000% Time: 427.8016s\n",
            "#75 Loss: 0.0032 Acc: 100.0000% Time: 433.5274s\n",
            "#76 Loss: 0.0019 Acc: 100.0000% Time: 439.0634s\n",
            "#77 Loss: 0.0013 Acc: 100.0000% Time: 444.8901s\n",
            "#78 Loss: 0.0030 Acc: 99.8665% Time: 450.4665s\n",
            "#79 Loss: 0.0020 Acc: 100.0000% Time: 456.4654s\n",
            "#80 Loss: 0.0010 Acc: 100.0000% Time: 462.2770s\n",
            "#81 Loss: 0.0009 Acc: 100.0000% Time: 468.4798s\n",
            "#82 Loss: 0.0011 Acc: 100.0000% Time: 474.0890s\n",
            "#83 Loss: 0.0078 Acc: 99.8665% Time: 479.7238s\n",
            "#84 Loss: 0.0027 Acc: 99.8665% Time: 485.5326s\n",
            "#85 Loss: 0.0027 Acc: 100.0000% Time: 491.2035s\n",
            "#86 Loss: 0.0018 Acc: 99.8665% Time: 496.8641s\n",
            "#87 Loss: 0.0094 Acc: 99.3324% Time: 502.6079s\n",
            "#88 Loss: 0.0084 Acc: 99.5995% Time: 508.1578s\n",
            "#89 Loss: 0.0187 Acc: 99.0654% Time: 514.1138s\n",
            "#90 Loss: 0.0092 Acc: 99.7330% Time: 519.7695s\n",
            "#91 Loss: 0.0021 Acc: 100.0000% Time: 525.2746s\n",
            "#92 Loss: 0.0030 Acc: 100.0000% Time: 531.0059s\n",
            "#93 Loss: 0.0027 Acc: 100.0000% Time: 536.6260s\n",
            "#94 Loss: 0.0112 Acc: 99.5995% Time: 542.3117s\n",
            "#95 Loss: 0.0105 Acc: 99.7330% Time: 547.8496s\n",
            "#96 Loss: 0.0100 Acc: 99.7330% Time: 553.8447s\n",
            "#97 Loss: 0.0058 Acc: 99.8665% Time: 559.4535s\n",
            "#98 Loss: 0.0059 Acc: 99.8665% Time: 565.1367s\n",
            "#99 Loss: 0.0067 Acc: 99.8665% Time: 571.1024s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2400 Acc: 94.3548% Time: 1.0384s\n",
            "현재 Best는 batch_szie : 9 / Loss : 0.1545006600478055 / Acc : 94.3548355102539\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# epochs = [50, 100, 150]\n",
        "for i in range(9, 10):\n",
        "    # set 9에서 j 번 째 학습을 의미\n",
        "    for batch in range(8, 13):\n",
        "        # 데이터셋을 불러올 때 사용할 변형(transformation) 객체 정의\n",
        "        # 이미지의 밝기(brightness), 대비(contrast), 채도(saturation), 색조(hue)를 일부 변경\n",
        "        transforms_train = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation)\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)\n",
        "        ])\n",
        "\n",
        "        transforms_test = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        data_dir = '/home/tutor/안재형/dog_pic/닥스훈트'\n",
        "        data_folder = os.path.join(data_dir, f'set_{i}')\n",
        "        train_datasets = datasets.ImageFolder(os.path.join(data_folder, 'train'), transforms_train)\n",
        "        test_datasets = datasets.ImageFolder(os.path.join(data_folder, 'test'), transforms_test)\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=batch, shuffle=True, num_workers=2)\n",
        "        test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=batch, shuffle=True, num_workers=2)\n",
        "\n",
        "#         print('학습 데이터셋 크기:', len(train_datasets))\n",
        "#         print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "        class_names = train_datasets.classes\n",
        "#         print('클래스:', class_names)\n",
        "\n",
        "        #--------------------------------------------#\n",
        "\n",
        "        model = models.resnet34(pretrained=True)\n",
        "        num_features = model.fc.in_features\n",
        "        # 전이 학습(transfer learning): 모델의 출력 뉴런 수를 2개로 교체하여 마지막 레이어 다시 학습\n",
        "        model.fc = nn.Linear(num_features, 2)\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "        #--------------------------------------------#\n",
        "        print(f'-------------현재 set_{i} batch_size는 {batch}-------------')\n",
        "        num_epochs = 100\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 전체 반복(epoch) 수 만큼 반복하며\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.\n",
        "            running_corrects = 0\n",
        "\n",
        "            # 배치 단위로 학습 데이터 불러오기\n",
        "            for inputs, labels in train_dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # 모델에 입력(forward)하고 결과 계산\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(train_datasets)\n",
        "            epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "\n",
        "            # 학습 과정 중에 결과 출력\n",
        "            print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "        #--------------------------------------------#\n",
        "\n",
        "        model.eval()\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0.\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in test_dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                # 한 배치의 첫 번째 이미지에 대하여 결과 시각화\n",
        "                print(f'[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})')\n",
        "    #             imshow(inputs.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
        "\n",
        "            epoch_loss = running_loss / len(test_datasets)\n",
        "            epoch_acc = running_corrects / len(test_datasets) * 100.\n",
        "            print('[Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "            Loss_Accuracy.append([i, batch, epoch_loss, epoch_acc])\n",
        "        \n",
        "        # 8*10개 모든 모델을 저장하지 않고 가장 좋은 성능의 모델을 저장\n",
        "        if batch == 8:\n",
        "            Best = [batch, epoch_loss, epoch_acc]\n",
        "            # 닥스훈트 set i / batch_size는 batch인 학습 2를 의미\n",
        "            torch.save(model, f'dac_set{i}_b{batch}_2.pth')\n",
        "\n",
        "        elif epoch_loss < Best[1]:\n",
        "            # Best 모델보다 Loss가 개선되면 저장\n",
        "            Best = [batch, epoch_loss, epoch_acc]\n",
        "            # 가장 마지막으로 저장된 모델이 제일 좋은 성능\n",
        "            torch.save(model, f'dac_set{i}_b{batch}_2.pth')\n",
        "        print(f'현재 Best는 batch_szie : {Best[0]} / Loss : {Best[1]} / Acc : {Best[2]}')\n",
        "    print()\n",
        "#         print('--------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13ab422d",
        "outputId": "25907587-e004-41e2-ed67-d256ba1a3fa4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[9, 1, 0.1535915126721281, tensor(93.5484, device='cuda:0')],\n",
              " [9, 1, 0.17863983664767968, tensor(93.9516, device='cuda:0')],\n",
              " [9, 2, 0.2933852207833326, tensor(91.1290, device='cuda:0')],\n",
              " [9, 3, 0.20758715865574026, tensor(94.3548, device='cuda:0')],\n",
              " [9, 4, 0.20669178503287125, tensor(93.9516, device='cuda:0')],\n",
              " [9, 5, 0.28108396162937305, tensor(94.3548, device='cuda:0')],\n",
              " [9, 6, 0.24342889199359038, tensor(93.5484, device='cuda:0')],\n",
              " [9, 7, 0.18509611624458264, tensor(95.1613, device='cuda:0')],\n",
              " [9, 8, 0.20722513832820808, tensor(91.9355, device='cuda:0')],\n",
              " [9, 9, 0.1858182937890712, tensor(95.5645, device='cuda:0')]]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Loss_Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 시각화"
      ],
      "metadata": {
        "id": "p1Ry3gx5DbsY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG7uKISzVbcA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sets = ['set 1', 'set 2', 'set 3', 'set 4', 'set 5',\n",
        "        'set 6', 'set 7', 'set 8']\n",
        "loss = [0.5013, 0.5006, 0.6272, 0.9832, 0.1888, 0.2433, 0.4016, 0.3315]\n",
        "acc  = [0.8000, 0.7560, 0.8040, 0.7000, 0.9480, 0.9440, 0.8760, 0.8951]\n",
        "\n",
        "sets = np.array(sets)\n",
        "loss = np.array(loss)\n",
        "acc = np.array(acc)\n",
        "\n",
        "df = pd.DataFrame({'set' : sets,\n",
        "                  'loss' : loss,\n",
        "                  'acc' : acc})\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "v10jlc2jOQ1U",
        "outputId": "e3c127e5-fc9e-4318-f6e9-989ae52200c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     set    loss     acc\n",
              "0  set 1  0.5013  0.8000\n",
              "1  set 2  0.5006  0.7560\n",
              "2  set 3  0.6272  0.8040\n",
              "3  set 4  0.9832  0.7000\n",
              "4  set 5  0.1888  0.9480\n",
              "5  set 6  0.2433  0.9440\n",
              "6  set 7  0.4016  0.8760\n",
              "7  set 8  0.3315  0.8951"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4cabf715-3611-4e89-89b0-ab220ad033f1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>set</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>set 1</td>\n",
              "      <td>0.5013</td>\n",
              "      <td>0.8000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>set 2</td>\n",
              "      <td>0.5006</td>\n",
              "      <td>0.7560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>set 3</td>\n",
              "      <td>0.6272</td>\n",
              "      <td>0.8040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>set 4</td>\n",
              "      <td>0.9832</td>\n",
              "      <td>0.7000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>set 5</td>\n",
              "      <td>0.1888</td>\n",
              "      <td>0.9480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>set 6</td>\n",
              "      <td>0.2433</td>\n",
              "      <td>0.9440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>set 7</td>\n",
              "      <td>0.4016</td>\n",
              "      <td>0.8760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>set 8</td>\n",
              "      <td>0.3315</td>\n",
              "      <td>0.8951</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4cabf715-3611-4e89-89b0-ab220ad033f1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4cabf715-3611-4e89-89b0-ab220ad033f1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4cabf715-3611-4e89-89b0-ab220ad033f1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss / acc\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.plot( df['loss'], color='green', linewidth=1.5, marker='o', label='Loss')\n",
        "plt.ylabel('Loss', color='green', labelpad = 10, fontsize=15)\n",
        "# plt.xlabel('Image Set', fontsize=15)\n",
        "plt.xticks( rotation = 20, fontsize=13)\n",
        "plt.ylim(0.1, 1.1)\n",
        "plt.legend(loc = 'upper left')\n",
        "\n",
        "ax=plt.gca()\n",
        "ax2=ax.twinx()\n",
        "\n",
        "ax2.bar(df.set, df['acc'],alpha=0.4 ,width=0.3, color='navy' , label = 'Accuracy' )\n",
        "ax2.set_ylabel('Accuracy', color='navy' , labelpad = 10 , fontsize=15)\n",
        "plt.ylim(0.6, 0.96)\n",
        "plt.legend(loc = 'upper right')\n",
        "\n",
        "plt.title('Loss and Accuracy', fontsize=15)\n",
        "plt.grid(True, axis='y', alpha=0.5, linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "zcqqFGN_QHWs",
        "outputId": "cb1cbb4e-ec74-429a-c2cd-096567e1c32f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFuYhpscYtGp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}